{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def find_submodules(module:nn.Module, condition):\n",
    "    def find(module, condition):\n",
    "        if condition(module): return [module] \n",
    "        else:                 return sum([find(o,condition) for o in module.children()], [])\n",
    "    return find(module,condition)\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x): return self.func(x)\n",
    "    \n",
    "def flatten(x):      return x.view(x.shape[0], -1)\n",
    "\n",
    "def get_cnn_layers2d(n_filters_pr_layer,  input_features, output_features, layer, **kwargs):\n",
    "    nfs = [input_features] + n_filters_pr_layer\n",
    "    print(f\"channels pr layers from input to output: {nfs+[output_features]}\")\n",
    "\n",
    "    in2hidden_layers     = [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] \n",
    "    print(f\"number of input and hidden layers: {len(in2hidden_layers)}\")\n",
    "\n",
    "    hidden2output_layers = [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "    print(f\"number of output layers :          {len(hidden2output_layers)}\")\n",
    "\n",
    "    all_layers = in2hidden_layers + hidden2output_layers\n",
    "    print(f\"total number of layers:            {len(all_layers)}\")\n",
    "    return all_layers\n",
    "    #return [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] + \\\n",
    "    #       [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "\n",
    "def get_cnn_model2d(filters_pr_layer,  input_features,  output_features, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers2d(filters_pr_layer,  input_features, output_features, layer, **kwargs))\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(x,self.leak, inplace=True) if self.leak is not None else F.relu(x, inplace=True)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "class ReLUOffset(torch.nn.ReLU):\n",
    "    def __init__(self, post_relu_offset = 0.15 ): #0.15915):\n",
    "        super().__init__(inplace=True)\n",
    "        self.register_buffer(\"offset\",torch.tensor(post_relu_offset, dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x+self.offset) - self.offset\n",
    "        return x\n",
    "\n",
    "class Maxout(nn.Module):\n",
    "    #here is the linear combination with the maxout\n",
    "\n",
    "    def __init__(self, pool_size, d_out=None):\n",
    "        super().__init__()\n",
    "        self.lin, self.pool_size, self.d_out = None, pool_size, d_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.shape = list(x.size())\n",
    "        if self.lin is None:\n",
    "            x        = x.view(self.shape[0],-1)\n",
    "            n_out    = x.shape[1] if self.d_out is None else self.d_out\n",
    "            self.lin = nn.Linear(x.shape[1], n_out * self.pool_size) \n",
    "            init.kaiming_uniform_(self.lin.weight, a=0, nonlinearity=\"relu\")\n",
    "            self.lin.bias.data.zero_()\n",
    "            #self.d_out = self.lin.weight.shape[0] // self.pool_size\n",
    "            #print(f\"shape:{shape} inputs.shape:{x.shape} self.lin.weight.shape:{self.lin.weight.shape}\")\n",
    "        if self.d_out is not None:\n",
    "            self.shape[-1] = self.d_out\n",
    "\n",
    "\n",
    "        x   = x.view(self.shape[0],-1)\n",
    "        #x_shape = x.shape\n",
    "        x   = self.lin(x)\n",
    "        #print(f\"0 before <=> after nn.linear x.shape:{x_shape} <=> x1.shape:{x1.shape}\")\n",
    "        x    = x.view(x.shape[0], x.shape[1] // self.pool_size, self.pool_size)\n",
    "        #print(f\"1 x.shape:{x.shape} \")\n",
    "        m, i = x.max(len(x.shape)-1)\n",
    "        m    = m.view(*self.shape)\n",
    "        #print(f\"m.shape:{m.shape} \")\n",
    "        #1/0\n",
    "        return m   \n",
    "def conv1d(ni, nf, ks=3, stride=1, bias=False, dropout_ratio=0.25):\n",
    "    if dropout_ratio > 0. :\n",
    "        #return [ torch.nn.Dropout(p=dropout_ratio, inplace=False),\n",
    "        #         torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "        return [ torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias),\n",
    "                 torch.nn.Dropout(p=dropout_ratio, inplace=False) ]\n",
    "    else:                    \n",
    "        return [torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "\n",
    "# 1 dimension model------------------------------\n",
    "\n",
    "def conv_layer1d(ni, nf, ks, stride, bn, zero_bn, act, dropout_ratio=0.15):\n",
    "    #ni:      number of input filters\n",
    "    #nf:      number of output filteres\n",
    "    #ks:      kernel size\n",
    "    #act:     activation function : nn.ReLU, nn.reLU\n",
    "    #bn:      create batchnorm layer\n",
    "    #zero_bn: init bias and weright in batchnorm to zero\n",
    "    layers = [*conv1d(ni, nf, ks, stride=stride, dropout_ratio=dropout_ratio)]\n",
    "    #\"\"\"\n",
    "    if bn: \n",
    "        bnorm = nn.BatchNorm1d(nf)\n",
    "        nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "        layers.append(bnorm)\n",
    "    #\"\"\"\n",
    "    if act is not None: \n",
    "        #layers.append(torch.nn.Dropout(p=dropout_ratio, inplace=False))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def get_cnn_layers1d(n_filters_pr_layer,  input_features, output_features, layer, **kwargs):\n",
    "    nfs = [input_features] + n_filters_pr_layer\n",
    "    print(f\"channels pr layers from input to output: {nfs+[output_features]}\")\n",
    "\n",
    "    in2hidden_layers     = [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] \n",
    "    print(f\"number of input and hidden layers: {len(in2hidden_layers)}\")\n",
    "\n",
    "    hidden2output_layers = [nn.AdaptiveAvgPool1d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "    print(f\"number of output layers :          {len(hidden2output_layers)}\")\n",
    "\n",
    "    all_layers = in2hidden_layers + hidden2output_layers\n",
    "    print(f\"total number of layers:            {len(all_layers)}\")\n",
    "    return all_layers\n",
    "    #return [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] + \\\n",
    "    #       [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "\n",
    "def get_cnn_model1d(filters_pr_layer,  input_features,  output_features, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers1d(filters_pr_layer,  input_features, output_features, layer, **kwargs))\n",
    "# ------------------------------------------------------------\n",
    "    \n",
    "# 2 dimension model-------------------------------------------\n",
    "def conv2d(ni, nf, ks=3, stride=1, bias=False, dropout_ratio=0.25):\n",
    "    if dropout_ratio > 0. :\n",
    "        #return [ torch.nn.Dropout(p=dropout_ratio, inplace=False),\n",
    "        #         torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "        return [ torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias),\n",
    "                 torch.nn.Dropout(p=dropout_ratio, inplace=False) ]\n",
    "    else:                    \n",
    "        return [torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "\n",
    "\n",
    "def conv_layer2(ni, nf, ks, stride, bn, zero_bn, act, dropout_ratio=0.15):\n",
    "    #ni:      number of input filters\n",
    "    #nf:      number of output filteres\n",
    "    #ks:      kernel size\n",
    "    #act:     activation function : nn.ReLU, nn.reLU\n",
    "    #bn:      create batchnorm layer\n",
    "    #zero_bn: init bias and weright in batchnorm to zero\n",
    "    layers = [*conv2d(ni, nf, ks, stride=stride, dropout_ratio=dropout_ratio)]\n",
    "    #\"\"\"\n",
    "    if bn: \n",
    "        bnorm = nn.BatchNorm2d(nf)\n",
    "        nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "        layers.append(bnorm)\n",
    "    #\"\"\"\n",
    "    if act is not None: \n",
    "        #layers.append(torch.nn.Dropout(p=dropout_ratio, inplace=False))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "# ------------------------------ 1 or 2 dimension model-------------------------------------------\n",
    "def conv(n_dim:int, ni, nf, ks=3, stride=1, bias=False, dropout_ratio=0.25):\n",
    "    #print(f\"conv n_dim:{n_dim} \")\n",
    "    if n_dim==1:\n",
    "        if dropout_ratio > 0. :\n",
    "            #return [ torch.nn.Dropout(p=dropout_ratio, inplace=False),\n",
    "            #         torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "            return [ torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias),\n",
    "                     torch.nn.Dropout(p=dropout_ratio, inplace=False) ]\n",
    "        else:                    \n",
    "            return [torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "    elif n_dim==2:    \n",
    "        if dropout_ratio > 0. :\n",
    "            #return [ torch.nn.Dropout(p=dropout_ratio, inplace=False),\n",
    "            #         torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "            return [ torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias),\n",
    "                     torch.nn.Dropout(p=dropout_ratio, inplace=False) ]\n",
    "        else:                    \n",
    "            return [torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "\n",
    "\n",
    "def conv_layer(n_dim:int, ni, nf, ks, stride, bn, zero_bn, act, dropout_ratio=0.15):\n",
    "    #print(f\"conv_layer n_dim:{n_dim} ni, nf, ks, stride, bn, zero_bn, {ni}, {nf}, {ks}, {stride}, {bn}, {zero_bn}\")\n",
    "    #ni:      number of input filters\n",
    "    #nf:      number of output filteres\n",
    "    #ks:      kernel size\n",
    "    #act:     activation function : nn.ReLU, nn.reLU\n",
    "    #bn:      create batchnorm layer\n",
    "    #zero_bn: init bias and weright in batchnorm to zero\n",
    "    if n_dim==1:\n",
    "        layers = [*conv(n_dim, ni, nf, ks, stride=stride, dropout_ratio=dropout_ratio)]\n",
    "        #\"\"\"\n",
    "        if bn: \n",
    "            bnorm = nn.BatchNorm1d(nf)\n",
    "            nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "            layers.append(bnorm)\n",
    "        #\"\"\"\n",
    "    elif n_dim==2:    \n",
    "        layers = [*conv(n_dim, ni, nf, ks, stride=stride, dropout_ratio=dropout_ratio)]\n",
    "        #\"\"\"\n",
    "        if bn: \n",
    "            bnorm = nn.BatchNorm2d(nf)\n",
    "            nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "            layers.append(bnorm)\n",
    "        #\"\"\"\n",
    "        \n",
    "    if act is not None: \n",
    "        #layers.append(torch.nn.Dropout(p=dropout_ratio, inplace=False))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def avgPool(n_dim:int, kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True):\n",
    "    #print(f\"avgPool n_dim:{n_dim} kernel_size:{kernel_size}, ceil_mode:{ceil_mode}, padding:{padding}\")\n",
    "    if n_dim==1:\n",
    "        pool = noop if stride==1 else nn.AvgPool1d(kernel_size, ceil_mode=ceil_mode, padding=padding)\n",
    "    elif n_dim==2:\n",
    "        pool = noop if stride==1 else nn.AvgPool2d(kernel_size, ceil_mode=ceil_mode, padding=padding)\n",
    "    return pool\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_dim, expansion, ni, nh, stride, activ_func, dpr=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        ni,nf   = ni*expansion, nh*expansion\n",
    "        print(f\"Resblock.__init__ n_dim:{n_dim} expansion:{expansion}, ni, nh, nf: {ni}, {nh}, {nf} stride:{stride}\")\n",
    "        layers  = [conv_layer(n_dim, ni, nh, ks=1, stride=1, bn=True, zero_bn=False, act=activ_func, dropout_ratio=dpr)]\n",
    "\n",
    "        if expansion==1 :\n",
    "            layers += [ conv_layer(n_dim, nh, nf, ks=3, stride=stride, bn=True, zero_bn=True,  act=activ_func, dropout_ratio=dpr) ] \n",
    "        else: \n",
    "            layers += [ conv_layer(n_dim, nh, nh, ks=3, stride=stride, bn=True, zero_bn=False, act=activ_func, dropout_ratio=dpr),\n",
    "                        conv_layer(n_dim, nh, nf, ks=1, stride=1,      bn=True, zero_bn=True,  act=activ_func, dropout_ratio=dpr) ]\n",
    "\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        self.pool   = avgPool(n_dim, nh, 2, ceil_mode=True, padding=0 if ni == 2*(ni//2) else 1)\n",
    "        #self.pool   = avgPool(n_dim, ni, 2, ceil_mode=True, padding=0 if ni == 2*(ni//2) else 1)\n",
    "        self.idconv = noop if ni==nf    else conv_layer(n_dim, ni, nf, ks=1, stride=1, bn=True, zero_bn=False, \n",
    "                                                        act=activ_func, dropout_ratio=dpr)\n",
    "\n",
    "        #self.dropout =   nn.Dropout(p=dpr)\n",
    "        #self.bn     = nn.BatchNorm2d(nf)\n",
    "\n",
    "        #self.bn_org = nn.BatchNorm2d(nf)\n",
    "        #self.act_fn_org  = activ_func() #nn.LeakyReLU(negative_slope=1e-2) #ReLUOffset()\n",
    "\n",
    "        self.act_fn  = activ_func() #nn.LeakyReLU(negative_slope=1e-2) #ReLUOffset()\n",
    "        #nn.init.constant_(self.bn.weight, .5)\n",
    "        #nn.init.constant_(self.bn.bias, 0.)\n",
    "        #nn.init.constant_(self.bn_org.weight, .5)\n",
    "        #nn.init.constant_(self.bn_org.bias, 0.)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        #return self.act_fn(self.convs(x).add_( self.idconv(self.pool(x))) )\n",
    "        #return self.act_fn( self.convs(x) + self.idconv(self.pool(x)) )\n",
    "\n",
    "        #x = self.convs(x) + self.act_fn_org( self.bn_org(self.idconv(self.pool(self.dropout(x))) ))\n",
    "        #return self.act_fn( self.bn(x) )\n",
    "\n",
    "        #return self.act_fn( self.convs(x) + self.idconv(self.pool(self.dropout(x))) ) \n",
    "        #print(f\"x.shape:{x.shape} self.pool(x):{self.pool(x).shape} kernel_size:{self.pool.kernel_size}\")\n",
    "        res = self.convs(x) + self.idconv(self.pool(x))\n",
    "        #print(f\"res:{res.shape}\")\n",
    "        \n",
    "        return res\n",
    "        #return self.act_fn( self.convs(x) + self.idconv(self.pool(x)) )\n",
    "        #return self.act_fn( self.bn(x) )\n",
    "\n",
    "\n",
    "class XResNet(nn.Sequential):\n",
    "    \n",
    "    #c_in=1 is configured for mnist\n",
    "    #@classmethod\n",
    "    #def create(cls, n_dim:int, expansion, layers, c_in=3, c_out=1000, activ_func = partial(nn.ReLU,inplace=True)):\n",
    "    def create(self, n_dim:int, expansion, layers, c_in=3, c_out=1000, activ_func = partial(nn.ReLU,inplace=True)):\n",
    "        self.c_out = c_out\n",
    "        dp = 0.15\n",
    "        nfs, strides, dpr  = ([c_in, (c_in+1)*8], [2,2], [dp,dp,dp]) if c_in==1 else  \\\n",
    "                             ([c_in, (c_in+1)*8, 32, 64], [1,2,2], [.1,dp,dp,dp])\n",
    "        \n",
    "        #print(f\"XResNet.create step 1 n_dim:{n_dim} dp:{dp}, nfs:{nfs}\")\n",
    "        if n_dim == 1:\n",
    "            stem    = [conv_layer(n_dim, nfs[i], nfs[i+1], ks=3, stride=strides[i], \n",
    "                                    act=activ_func, bn=True, zero_bn=False, dropout_ratio=dpr[i]) \n",
    "                       for i in range(len(nfs)-1)]\n",
    "        elif n_dim == 2:\n",
    "            stem    = [conv_layer(n_dim, nfs[i], nfs[i+1], ks=3, stride=strides[i], \n",
    "                                    act=activ_func, bn=True, zero_bn=False, dropout_ratio=dpr[i]) \n",
    "                       for i in range(len(nfs)-1)]\n",
    "\n",
    "        #nfs, strides  = ([16//expansion,16,32,32],       [1,2,1]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([16//expansion,64,128],      [2,2,1]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([32//expansion,64,128],      [2,2,1]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([32//expansion,64,128],         [2,2]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([32//expansion,64,128,256],     [2,1,2]) if c_in==1 else  \\\n",
    "        \n",
    "        #nfs, strides  = ([32//expansion,64,256],         [2,2]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([16//expansion,32,64,128],     [2,2,2]) if c_in==1 else  \\\n",
    "        nfs, strides  = ([16//expansion,64,256,256],     [2,2,2]) if c_in==1 else  \\\n",
    "                        ([64//expansion,64,128,256,512], [1,2,2,2]) \n",
    "        #print(f\"XResNet.create step 2 nfs:{nfs} layers:{layers}\")\n",
    "        if len(nfs) < len(layers)+1: layers=layers[0:len(nfs)-1]\n",
    "        res_layers = [XResNet._make_layer(n_dim, expansion, nfs[i], nfs[i+1], n_blocks=l, stride=strides[i], \n",
    "                                             activ_func=activ_func,dpr=dp) for i,l in enumerate(layers) ]\n",
    "\n",
    "        self.stems = nn.Sequential(*stem)\n",
    "        self.residuallayers = nn.Sequential(*res_layers)\n",
    "        self.flatten = Flatten()\n",
    "        self.dense = None\n",
    "        return self\n",
    "    \n",
    "        \"\"\"\n",
    "        res = cls(\n",
    "            *stem,\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            #nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *res_layers,\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            #nn.AdaptiveAvgPool2d(1), \n",
    "            Flatten(),\n",
    "            #nn.BatchNorm1d(n2_res),\n",
    "            nn.Linear(n2_res, c_out)\n",
    "        )\n",
    "        return res\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "            nn.Dropout(p=dp, inplace=False),\n",
    "            nn.Linear(n2_res, n2),\n",
    "            nn.BatchNorm1d(n2),\n",
    "            activ_func(),\n",
    "\n",
    "            nn.BatchNorm1d(n2),\n",
    "            nn.Dropout(p=dp, inplace=False),\n",
    "            nn.Linear(n2, c_out)\n",
    "        return res\n",
    "        \"\"\"\n",
    "    \n",
    "    def forward(self, x): \n",
    "        #print(f\"x:{x.shape}\")\n",
    "        r1 = self.stems(x)\n",
    "        #print(f\"r1:{r1.shape}\")\n",
    "        r2 = self.residuallayers(r1)\n",
    "        #print(f\"r2:{r2.shape}\")\n",
    "        r3 = self.flatten(r2)\n",
    "        #print(f\"r3:{r3.shape}\")\n",
    "        \n",
    "        if self.dense is None : \n",
    "            self.dense = nn.Linear(r3.shape[-1], self.c_out)\n",
    "            \n",
    "        r4 = self.dense(r3)\n",
    "        #print(f\"self.dense.weight.shape:{self.dense.weight.shape} r4:{r4.shape}\")\n",
    "        return r4\n",
    "        \n",
    "    @staticmethod\n",
    "    def _make_layer( n_dim:int, expansion, ni, nf, n_blocks, stride, activ_func,dpr):\n",
    "        blocks = [ResBlock(n_dim, expansion, (ni if i==0 else nf), nf, stride=stride, \n",
    "                           activ_func=activ_func,dpr=dpr) for i in range(n_blocks)]   \n",
    "        return nn.Sequential(*blocks)   \n",
    "\n",
    "    def initialize(self, uniform:bool=False, a=0., nonlinearity=\"relu\"):\n",
    "        modules = find_submodules(self, lambda m: not isinstance(m, nn.Sequential))\n",
    "        print(modules)\n",
    "        for m in modules:\n",
    "            #print(f\"tensor.shape: {m.shape}\")\n",
    "            if isinstance(m, (nn.Conv2d,nn.Linear) ) or isinstance(m, (nn.Conv1d,nn.Linear) ):\n",
    "                init.kaiming_uniform_(m.weight, a=a, nonlinearity=nonlinearity) if uniform else \\\n",
    "                init.kaiming_normal_( m.weight, a=a, nonlinearity=nonlinearity)\n",
    "                if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "            #elif: isinstance(m, nn.BatchNorm2d):\n",
    "            #    nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "\n",
    "            if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "\n",
    "def xresnet18 (n_dim:int, **kwargs): return XResNet().create(n_dim, expansion=1, layers=[2, 2, 2,  2], **kwargs)\n",
    "def xresnet34 (n_dim:int, **kwargs): return XResNet().create(n_dim, expansion=1, layers=[3, 4, 6,  3], **kwargs)\n",
    "def xresnet50 (n_dim:int, **kwargs): return XResNet().create(n_dim, expansion=4, layers=[3, 4, 6,  3], **kwargs)\n",
    "def xresnet101(n_dim:int, **kwargs): return XResNet().create(n_dim, expansion=4, layers=[3, 4, 23, 3], **kwargs)\n",
    "def xresnet152(n_dim:int, **kwargs): return XResNet().create(n_dim, expansion=4, layers=[3, 8, 36, 3], **kwargs)   \n",
    "#------------------------------\n",
    "################### top is refactored from the code below this line #####################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted app_image_02_imagenette_optimizers.ipynb.\n",
      "Converted fin_01_candlestick.ipynb.\n",
      "Converted fin_02_simfin_data.ipynb.\n",
      "Converted fin_02_simfin_generated_data.ipynb.\n",
      "Converted fin_02_simfin_training.ipynb.\n",
      "Converted fin_03_graphs.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted parallel.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
