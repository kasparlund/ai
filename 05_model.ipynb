{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch import nn\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def find_submodules(module:nn.Module, condition):\n",
    "    def find(module, condition):\n",
    "        if condition(module): return [module] \n",
    "        else:                 return sum([find(o,condition) for o in module.children()], [])\n",
    "    return find(module,condition)\n",
    "\n",
    "class Lambda(nn.Module):\n",
    "    def __init__(self, func):\n",
    "        super().__init__()\n",
    "        self.func = func\n",
    "    def forward(self, x): return self.func(x)\n",
    "    \n",
    "def flatten(x):      return x.view(x.shape[0], -1)\n",
    "\n",
    "def get_cnn_layers(n_filters_pr_layer,  input_features, output_features, layer, **kwargs):\n",
    "    nfs = [input_features] + n_filters_pr_layer\n",
    "    print(f\"channels pr layers from input to output: {nfs+[output_features]}\")\n",
    "\n",
    "    in2hidden_layers     = [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] \n",
    "    print(f\"number of input and hidden layers: {len(in2hidden_layers)}\")\n",
    "\n",
    "    hidden2output_layers = [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "    print(f\"number of output layers :          {len(hidden2output_layers)}\")\n",
    "\n",
    "    all_layers = in2hidden_layers + hidden2output_layers\n",
    "    print(f\"total number of layers:            {len(all_layers)}\")\n",
    "    return all_layers\n",
    "    #return [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] + \\\n",
    "    #       [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "\n",
    "def get_cnn_model(filters_pr_layer,  input_features,  output_features, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers(filters_pr_layer,  input_features, output_features, layer, **kwargs))\n",
    "\n",
    "def noop(x): return x\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class GeneralRelu(nn.Module):\n",
    "    def __init__(self, leak=None, sub=None, maxv=None):\n",
    "        super().__init__()\n",
    "        self.leak,self.sub,self.maxv = leak,sub,maxv\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(x,self.leak, inplace=True) if self.leak is not None else F.relu(x, inplace=True)\n",
    "        if self.sub is not None: x.sub_(self.sub)\n",
    "        if self.maxv is not None: x.clamp_max_(self.maxv)\n",
    "        return x\n",
    "\n",
    "class ReLUOffset(torch.nn.ReLU):\n",
    "    def __init__(self, post_relu_offset = 0.15 ): #0.15915):\n",
    "        super().__init__(inplace=True)\n",
    "        self.register_buffer(\"offset\",torch.tensor(post_relu_offset, dtype=torch.float32))\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x+self.offset) - self.offset\n",
    "        return x\n",
    "\n",
    "class Maxout(nn.Module):\n",
    "    #here is the linear combination with the maxout\n",
    "\n",
    "    def __init__(self, pool_size, d_out=None):\n",
    "        super().__init__()\n",
    "        self.lin, self.pool_size, self.d_out = None, pool_size, d_out\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.shape = list(x.size())\n",
    "        if self.lin is None:\n",
    "            x        = x.view(self.shape[0],-1)\n",
    "            n_out    = x.shape[1] if self.d_out is None else self.d_out\n",
    "            self.lin = nn.Linear(x.shape[1], n_out * self.pool_size) \n",
    "            init.kaiming_uniform_(self.lin.weight, a=0, nonlinearity=\"relu\")\n",
    "            self.lin.bias.data.zero_()\n",
    "            #self.d_out = self.lin.weight.shape[0] // self.pool_size\n",
    "            #print(f\"shape:{shape} inputs.shape:{x.shape} self.lin.weight.shape:{self.lin.weight.shape}\")\n",
    "        if self.d_out is not None:\n",
    "            self.shape[-1] = self.d_out\n",
    "\n",
    "\n",
    "        x   = x.view(self.shape[0],-1)\n",
    "        #x_shape = x.shape\n",
    "        x   = self.lin(x)\n",
    "        #print(f\"0 before <=> after nn.linear x.shape:{x_shape} <=> x1.shape:{x1.shape}\")\n",
    "        x    = x.view(x.shape[0], x.shape[1] // self.pool_size, self.pool_size)\n",
    "        #print(f\"1 x.shape:{x.shape} \")\n",
    "        m, i = x.max(len(x.shape)-1)\n",
    "        m    = m.view(*self.shape)\n",
    "        #print(f\"m.shape:{m.shape} \")\n",
    "        #1/0\n",
    "        return m   \n",
    "def conv1(ni, nf, ks=3, stride=1, bias=False, dropout_ratio=0.25):\n",
    "    if dropout_ratio > 0. :\n",
    "        #return [ torch.nn.Dropout(p=dropout_ratio, inplace=False),\n",
    "        #         torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "        return [ torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias),\n",
    "                 torch.nn.Dropout(p=dropout_ratio, inplace=False) ]\n",
    "    else:                    \n",
    "        return [torch.nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "\n",
    "# 1 dimension model------------------------------\n",
    "\n",
    "def conv_layer1(ni, nf, ks, stride, bn, zero_bn, act, dropout_ratio=0.15):\n",
    "    #ni:      number of input filters\n",
    "    #nf:      number of output filteres\n",
    "    #ks:      kernel size\n",
    "    #act:     activation function : nn.ReLU, nn.reLU\n",
    "    #bn:      create batchnorm layer\n",
    "    #zero_bn: init bias and weright in batchnorm to zero\n",
    "    layers = [*conv1(ni, nf, ks, stride=stride, dropout_ratio=dropout_ratio)]\n",
    "    #\"\"\"\n",
    "    if bn: \n",
    "        bnorm = nn.BatchNorm1d(nf)\n",
    "        nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "        layers.append(bnorm)\n",
    "    #\"\"\"\n",
    "    if act is not None: \n",
    "        #layers.append(torch.nn.Dropout(p=dropout_ratio, inplace=False))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def get_cnn_layers1(n_filters_pr_layer,  input_features, output_features, layer, **kwargs):\n",
    "    print(\"get_cnn_layers\")\n",
    "    \n",
    "    nfs = [input_features] + n_filters_pr_layer\n",
    "    print(f\"channels pr layers from input to output: {nfs+[output_features]}\")\n",
    "\n",
    "    in2hidden_layers     = [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] \n",
    "    print(f\"number of input and hidden layers: {len(in2hidden_layers)}\")\n",
    "\n",
    "    hidden2output_layers = [nn.AdaptiveAvgPool1d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "    print(f\"number of output layers :          {len(hidden2output_layers)}\")\n",
    "\n",
    "    all_layers = in2hidden_layers + hidden2output_layers\n",
    "    print(f\"total number of layers:            {len(all_layers)}\")\n",
    "    return all_layers\n",
    "    #return [layer(nfs[i], nfs[i+1], ks=(5 if i==0 else 3), **kwargs) for i in range(len(nfs)-1)] + \\\n",
    "    #       [nn.AdaptiveAvgPool2d(1), Lambda(flatten), nn.Linear(nfs[-1], output_features)]\n",
    "\n",
    "def get_cnn_model1(filters_pr_layer,  input_features,  output_features, layer, **kwargs):\n",
    "    return nn.Sequential(*get_cnn_layers1(filters_pr_layer,  input_features, output_features, layer, **kwargs))\n",
    "# ------------------------------------------------------------\n",
    "    \n",
    "# 2 dimension model-------------------------------------------\n",
    "def conv2(ni, nf, ks=3, stride=1, bias=False, dropout_ratio=0.25):\n",
    "    if dropout_ratio > 0. :\n",
    "        #return [ torch.nn.Dropout(p=dropout_ratio, inplace=False),\n",
    "        #         torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "        return [ torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias),\n",
    "                 torch.nn.Dropout(p=dropout_ratio, inplace=False) ]\n",
    "    else:                    \n",
    "        return [torch.nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)]\n",
    "\n",
    "\n",
    "def conv_layer2(ni, nf, ks, stride, bn, zero_bn, act, dropout_ratio=0.15):\n",
    "    #ni:      number of input filters\n",
    "    #nf:      number of output filteres\n",
    "    #ks:      kernel size\n",
    "    #act:     activation function : nn.ReLU, nn.reLU\n",
    "    #bn:      create batchnorm layer\n",
    "    #zero_bn: init bias and weright in batchnorm to zero\n",
    "    layers = [*conv2(ni, nf, ks, stride=stride, dropout_ratio=dropout_ratio)]\n",
    "    #\"\"\"\n",
    "    if bn: \n",
    "        bnorm = nn.BatchNorm2d(nf)\n",
    "        nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "        layers.append(bnorm)\n",
    "    #\"\"\"\n",
    "    if act is not None: \n",
    "        #layers.append(torch.nn.Dropout(p=dropout_ratio, inplace=False))\n",
    "        layers.append(act())\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, expansion, ni, nh, stride, activ_func, dpr=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        ni,nf   = ni*expansion, nh*expansion\n",
    "        layers  = [conv_layer2(ni, nh, 1, stride=1,      bn=True, zero_bn=False, act=activ_func, dropout_ratio=dpr)]\n",
    "\n",
    "        if expansion==1 :\n",
    "            layers += [ conv_layer2(nh, nf, 3, stride=stride, bn=True, zero_bn=True,  act=activ_func, dropout_ratio=dpr) ] \n",
    "        else: \n",
    "            layers += [ conv_layer2(nh, nh, 3, stride=stride, bn=True, zero_bn=False, act=activ_func, dropout_ratio=dpr),\n",
    "                        conv_layer2(nh, nf, 1, stride=1,      bn=True, zero_bn=True,  act=activ_func, dropout_ratio=dpr) ]\n",
    "\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        #print(f\"expansion, stride, ni==nf:{expansion}, {stride}, {ni}, {nf}\")\n",
    "        self.pool   = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True, padding=0 if ni == 2*(ni//2) else 1)\n",
    "        self.idconv = noop if ni==nf    else conv_layer2(ni, nf, 1, stride=1, bn=True, zero_bn=False, act=activ_func, dropout_ratio=dpr)\n",
    "\n",
    "        #self.dropout =   nn.Dropout(p=dpr)\n",
    "        #self.bn     = nn.BatchNorm2d(nf)\n",
    "\n",
    "        #self.bn_org = nn.BatchNorm2d(nf)\n",
    "        #self.act_fn_org  = activ_func() #nn.LeakyReLU(negative_slope=1e-2) #ReLUOffset()\n",
    "\n",
    "        self.act_fn  = activ_func() #nn.LeakyReLU(negative_slope=1e-2) #ReLUOffset()\n",
    "        #nn.init.constant_(self.bn.weight, .5)\n",
    "        #nn.init.constant_(self.bn.bias, 0.)\n",
    "        #nn.init.constant_(self.bn_org.weight, .5)\n",
    "        #nn.init.constant_(self.bn_org.bias, 0.)\n",
    "\n",
    "    def forward(self, x): \n",
    "        #return self.act_fn(self.convs(x).add_( self.idconv(self.pool(x))) )\n",
    "        #return self.act_fn( self.convs(x) + self.idconv(self.pool(x)) )\n",
    "\n",
    "        #x = self.convs(x) + self.act_fn_org( self.bn_org(self.idconv(self.pool(self.dropout(x))) ))\n",
    "        #return self.act_fn( self.bn(x) )\n",
    "\n",
    "        #return self.act_fn( self.convs(x) + self.idconv(self.pool(self.dropout(x))) ) \n",
    "\n",
    "        return self.convs(x) + self.idconv(self.pool(x))\n",
    "        #return self.act_fn( self.convs(x) + self.idconv(self.pool(x)) )\n",
    "        #return self.act_fn( self.bn(x) )\n",
    "\n",
    "\n",
    "class XResNet(nn.Sequential):\n",
    "    #c_in=1 is configured for mnist\n",
    "    @classmethod\n",
    "    def create(cls, expansion, layers, c_in=3, c_out=1000, activ_func = partial(nn.ReLU,inplace=True)):\n",
    "        dp = 0.15\n",
    "        #nfs, strides, dpr  = ([c_in, (c_in+1)*8],         [2],    [.0,.15]) if c_in==1 else  \\\n",
    "        #nfs, strides, dpr  = ([c_in, (c_in+1)*8, 32],     [1,2],  [.0,.15,.15]) if c_in==1 else  \\\n",
    "        #nfs, strides, dpr  = ([c_in, (c_in+1)*8, 16], [1,2,1],  [.0,.15,.15]) if c_in==1 else  \\\n",
    "        #nfs, strides, dpr  = ([c_in, (c_in+1)*8, 32, 32], [1,2,1], [dp,dp,dp]) if c_in==1 else  \\\n",
    "        nfs, strides, dpr  = ([c_in, (c_in+1)*8, 32], [1,2], [dp,dp,dp]) if c_in==1 else  \\\n",
    "                             ([c_in, (c_in+1)*8, 32, 64], [1,2,2], [.1,dp,dp,dp])\n",
    "        stem    = [conv_layer2( nfs[i], nfs[i+1], ks=3, stride=strides[i], \n",
    "                               act=activ_func, bn=True, zero_bn=False, dropout_ratio=dpr[i]) for i in range(len(nfs)-1)]\n",
    "\n",
    "        #nfs, strides  = ([16//expansion,16,32,32],       [1,2,1]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([16//expansion,64,128],      [2,2,1]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([32//expansion,64,128],      [2,2,1]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([32//expansion,64,128],         [2,2]) if c_in==1 else  \\\n",
    "        #nfs, strides  = ([32//expansion,64,128,256],     [2,1,2]) if c_in==1 else  \\\n",
    "        nfs, strides  = ([32//expansion,64,256],     [2,2]) if c_in==1 else  \\\n",
    "                        ([64//expansion,64,128,256,512], [1,2,2,2]) \n",
    "        if len(nfs) < len(layers)+1: layers=layers[0:len(nfs)-1]\n",
    "        res_layers = [cls._make_layer(expansion, nfs[i], nfs[i+1], n_blocks=l, stride=strides[i], \n",
    "                                      activ_func=activ_func,dpr=dp) for i,l in enumerate(layers) ]\n",
    "\n",
    "        #n2 =  int(0.5+0.5*(nfs[-1]*expansion+c_out))  \n",
    "        n2_res = nfs[-1]*expansion                                   \n",
    "        n2 =  int( 0.5*(nfs[-1]*expansion+c_out))\n",
    "        res = cls(\n",
    "            *stem,\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            #nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            *res_layers,\n",
    "            #nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "            #nn.AdaptiveAvgPool2d(1), \n",
    "            Flatten(),\n",
    "            #nn.BatchNorm1d(n2_res),\n",
    "            nn.Linear(n2_res, c_out)\n",
    "        )\n",
    "        \"\"\"\n",
    "            nn.Dropout(p=dp, inplace=False),\n",
    "            nn.Linear(n2_res, n2),\n",
    "            nn.BatchNorm1d(n2),\n",
    "            activ_func(),\n",
    "\n",
    "            nn.BatchNorm1d(n2),\n",
    "            nn.Dropout(p=dp, inplace=False),\n",
    "            nn.Linear(n2, c_out)\n",
    "        \"\"\"\n",
    "        return res\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layer(expansion, ni, nf, n_blocks, stride, activ_func,dpr):\n",
    "        blocks = [ResBlock(expansion, (ni if i==0 else nf), nf, stride=stride, \n",
    "                           activ_func=activ_func,dpr=dpr) for i in range(n_blocks)]   \n",
    "        return nn.Sequential(*blocks)   \n",
    "\n",
    "    def initialize(self, uniform:bool=False, a=0., nonlinearity=\"relu\"):\n",
    "        modules = find_submodules(self, lambda m: not isinstance(m, nn.Sequential))\n",
    "        for m in modules:\n",
    "            if isinstance(m, (nn.Conv2d,nn.Linear) ) or isinstance(m, (nn.Conv1d,nn.Linear) ):\n",
    "                init.kaiming_uniform_(m.weight, a=a, nonlinearity=nonlinearity) if uniform else \\\n",
    "                init.kaiming_normal_( m.weight, a=a, nonlinearity=nonlinearity)\n",
    "                if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "            #elif: isinstance(m, nn.BatchNorm2d):\n",
    "            #    nn.init.constant_(bnorm.weight, 0. if zero_bn else 1.)\n",
    "\n",
    "            if getattr(m, 'bias', None) is not None: m.bias.data.zero_()\n",
    "\n",
    "def xresnet18 (**kwargs): return XResNet.create(1, [2, 2, 2,  2], **kwargs)\n",
    "def xresnet34 (**kwargs): return XResNet.create(1, [3, 4, 6,  3], **kwargs)\n",
    "def xresnet50 (**kwargs): return XResNet.create(4, [3, 4, 6,  3], **kwargs)\n",
    "def xresnet101(**kwargs): return XResNet.create(4, [3, 4, 23, 3], **kwargs)\n",
    "def xresnet152(**kwargs): return XResNet.create(4, [3, 8, 36, 3], **kwargs)   \n",
    "#------------------------------\n",
    "################### top is refactored from the code below this line #####################\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#class Conv2d_cai (torch.nn.Conv2d):\n",
    "#    def __init__ (in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, #padding_mode='zeros', dropout_ratio=-1):\n",
    "#        super().__init__(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, #stride=stride, padding=padding, \n",
    "#        dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)\n",
    "#        self.register_buffer(\"dropout_ratio\",self.dropout_ratio)\n",
    "#        \n",
    "#   def forward(self, x): \n",
    "#        return super().forward(x)\n",
    "#   #print(dict(learn.model.named_buffers()).keys())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted app_image_02_imagenette_optimizers.ipynb.\n",
      "Converted fin_01_candlestick.ipynb.\n",
      "Converted fin_02_simfin_data-Copy1.ipynb.\n",
      "Converted fin_02_simfin_data.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
