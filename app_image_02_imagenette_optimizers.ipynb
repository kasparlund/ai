{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notebook imagenette\n",
    "Need to make pytorch transforms work.\n",
    "To begin with crop image must be made image can in af databaunch can be scale to the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from torch import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#let me be on my mac\n",
    "def cuda(self, device=None, non_blocking=False) : return self\n",
    "torch.Tensor.cuda = cuda\n",
    "\n",
    "from lib.data.external import *\n",
    "from lib.data.lists import *\n",
    "from lib.learner.learner import *\n",
    "from lib.learner.optimizers import *\n",
    "from lib.image.image import *\n",
    "from lib.model.model import*\n",
    "from lib.model.modelmanager import*\n",
    "\n",
    "from functools import *\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imagenette data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#path = datasets.untar_data(datasets.URLs.IMAGENETTE_160)\n",
    "path = Path(\"/Users/kasparlund/.fastai/data/imagenette-160\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tfms = [make_rgb, ResizeFixed(128), to_byte_tensor, to_float_tensor]\n",
    "bs=128\n",
    "\n",
    "images = ImageList.from_files(path, tfms=tfms)\n",
    "#sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='val'))\n",
    "#ll = label_by_func(sd, parent_labeler)\n",
    "#sd = SplitData.split_by_func(images, splitter); \n",
    "\n",
    "#split data based on the foldername of the grandparent\n",
    "splitter    = partial(grandparent_splitter, valid_name='val')\n",
    "sd          = SplitData.split_by_func(images, splitter); \n",
    "data = label_train_valid_data(sd, parent_labeler, proc_y=CategoryProcessor())\n",
    "\n",
    "train_dl,valid_dl = ( DataLoader(data.train, batch_size=bs,   num_workers=4, shuffle=True),\n",
    "                      DataLoader(data.valid, batch_size=bs*2, num_workers=4))\n",
    "databunch = DataBunch(train_dl, valid_dl, c_in=3, c_out=max(data.train.y)+1)\n",
    "\n",
    "#train_dl,valid_dl = ( DataLoader(ll.train, batch_size=bs,   num_workers=4, shuffle=True),\n",
    "#                      DataLoader(ll.valid, batch_size=bs*2, num_workers=4))\n",
    "#data = DataBunch(train_dl, valid_dl, c_in=3, c_out=max(ll.train.y)+1)\n",
    "#print(len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13394, 9469, 3925, 74, 8)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "config   = IMAGENETTE_160_Configuration()\n",
    "inputTfm = Transforms(Image2TensorImage(), NormalizeTransform(config.scale,config.mean,config.std))\n",
    "files    = ImageList.from_files( untar_data(config.url) )\n",
    "#ds       = ImageDataset(config, files, input_transform=inputTfm, output_transform=Numpy2Tensor(dtype=int)  )\n",
    "ds       = ImageDataset(config, files, input_transform=inputTfm )\n",
    "ds_train, ds_test = ds.split2train_test()\n",
    "\n",
    "dl_train, dl_test = ds_train.dataloader(128,True), ds_test.dataloader(512, False)\n",
    "databunch = DataBunch(dl_train, dl_test, config.channels_in, config.channels_out)\n",
    "\n",
    "len(ds), len(ds_train), len(ds_test),len(dl_train), len(dl_test), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[255., 250., 255.,  ..., 254., 254., 255.],\n",
       "          [255., 242., 251.,  ..., 255., 255., 255.],\n",
       "          [255., 252., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [249., 255., 255.,  ..., 254., 255., 254.],\n",
       "          [255., 254., 255.,  ..., 254., 254., 255.],\n",
       "          [255., 247., 254.,  ..., 253., 254., 255.]],\n",
       " \n",
       "         [[255., 250., 255.,  ..., 254., 254., 255.],\n",
       "          [255., 242., 251.,  ..., 255., 255., 255.],\n",
       "          [255., 252., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [248., 255., 255.,  ..., 254., 255., 254.],\n",
       "          [255., 253., 255.,  ..., 254., 254., 255.],\n",
       "          [255., 246., 253.,  ..., 253., 254., 255.]],\n",
       " \n",
       "         [[255., 250., 255.,  ..., 254., 254., 255.],\n",
       "          [255., 242., 251.,  ..., 255., 255., 255.],\n",
       "          [255., 252., 255.,  ..., 255., 255., 255.],\n",
       "          ...,\n",
       "          [246., 253., 253.,  ..., 254., 255., 254.],\n",
       "          [253., 251., 253.,  ..., 254., 254., 255.],\n",
       "          [253., 244., 251.,  ..., 253., 254., 255.]]]),\n",
       " 0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#layers_sizes = [32,64,128,256,512]\n",
    "layers_sizes = [64,64,128,256]\n",
    "\n",
    "sched = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) \n",
    "cbfs_base = [TrainableModelCallback, TrainEvalCallback, OptimizerCallback, \n",
    "#        partial(ParamScheduler, 'lr', sched),\n",
    "        partial(BatchTransformXCallback, view_tfm(3,64,64)),\n",
    "        partial(MixUp,Î±=0.4),\n",
    "        \n",
    "        #CudaCallback,\n",
    "        ProgressCallback,\n",
    "       ]\n",
    "cbfs = cbfs_base.copy() + [Recorder, partial(AvgStatsCallback,[accuracy])]\n",
    "#cbfs_lr_Finder = cbfs_base.copy() + [LR_Finder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For equal distribution of cases pr class. nb_classes:-1: \n",
      "initial loss:    nan\n",
      "initial accuracy -1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-8b36cdff0019>:3: RuntimeWarning: invalid value encountered in log\n",
      "  initial_loss = -np.log( 1.0/nb_classes )\n"
     ]
    }
   ],
   "source": [
    "# average loss pr input sample at iteration 0\n",
    "nb_classes = config.channels_out\n",
    "initial_loss = -np.log( 1.0/nb_classes ) \n",
    "print(f\"For equal distribution of cases pr class. nb_classes:{nb_classes}: \\ninitial loss:    {initial_loss}\\ninitial accuracy {1.0/nb_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steppers=[sgd_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels pr layers from input to output: [3, 64, 64, 128, 256, -1]\n",
      "number of input and hidden layers: 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -1: [-1, 256]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-93344e79bf54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_bn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmm\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mCnnModelManager\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mget_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabunch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_resnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabunch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/AICodeData/ai/lib/model/model.py\u001b[0m in \u001b[0;36mget_cnn_model\u001b[0;34m(filters_pr_layer, input_features, output_features, layer, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_cnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters_pr_layer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0moutput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mget_cnn_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters_pr_layer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/code/AICodeData/ai/lib/model/model.py\u001b[0m in \u001b[0;36mget_cnn_layers\u001b[0;34m(n_filters_pr_layer, input_features, output_features, layer, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"number of input and hidden layers: {len(in2hidden_layers)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mhidden2output_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdaptiveAvgPool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"number of output layers :          {len(hidden2output_layers)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/financeNB/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_features, out_features, bias)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -1: [-1, 256]"
     ]
    }
   ],
   "source": [
    "layer = partial(conv_layer2, stride=2, bn=False, zero_bn=False, act=torch.nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "\n",
    "learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "%time learn.fit(1, opt=SGD(sched,max_lr=0.1), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "epoch\ttrain_loss\ttrain_accuracy\tvalid_loss\tvalid_accuracy\ttime\n",
    "0\t2.009067\t0.298201\t1.793345\t0.384000\t01:33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(skip_end=15),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## steppers=[weight_decay, sgd_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = partial(conv_layer, stride=2, bn=False, zero_bn=False, act=nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "\n",
    "learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "#print(opt.hypers[0]['lr']), print(opt.hypers[0]['wd'])\n",
    "%time learn.fit(1, opt=SGD(sched,max_lr=0.1, max_wd=1e-4), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## momentum: steppers=[momentum_step,weight_decay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = partial(conv_layer, stride=2, bn=False, zero_bn=False, act=nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "\n",
    "learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "%time learn.fit(1, opt=SGD_Momentum(sched,max_lr=0.08, moms=(0.85,0.95), max_wd=2e-4 ), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(skip_end=15),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam: steppers=[adam_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = partial(conv_layer, stride=2, bn=False, zero_bn=False, act=nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "                        \n",
    "learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "%time learn.fit(1, opt=Adam(sched,max_lr=3e-4, moms=(0.85,0.95), max_wd = 1e-6), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(skip_end=15),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB: steppers=[lamb_step]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = partial(conv_layer, stride=2, bn=False, zero_bn=False, act=nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "                        \n",
    "learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "%time learn.fit(3, opt=LAMB(sched,max_lr=3e-3, moms=(0.85,0.95), max_wd = 1e-6), cb_funcs=cbfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.find_subcription_by_cls(LR_Finder).plot_loss(skip_end=17),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_lr(),plt.show()\n",
    "learn.find_subcription_by_cls(Recorder).plot_loss(),plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam with hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = partial(conv_layer, stride=2, bn=False, zero_bn=False, act=nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "                        \n",
    "with Hooks(mm.model, append_stats) as hooks: \n",
    "    learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "    learn.fit(1, opt=Adam(sched,max_lr=2e-4, moms=(0.85,0.95), max_wd = 1e-4), cb_funcs=cbfs)\n",
    "    plot_layer_stats( hooks )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAMB with hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = partial(conv_layer, stride=2, bn=False, zero_bn=False, act=nn.ReLU )\n",
    "mm    = CnnModelManager( get_cnn_model(layers_sizes, databunch.c_in, databunch.c_out, layer ) )\n",
    "mm.initialize(is_resnet=False)\n",
    "\n",
    "with Hooks(mm.model, append_stats) as hooks: \n",
    "    learn = Learner( mm.model, databunch, loss_func=F.cross_entropy)\n",
    "    learn.fit(1, opt=LAMB(sched,max_lr=1e-2, moms=(0.85,0.95), max_wd = 1e-6), cb_funcs=cbfs )\n",
    "    plot_layer_stats( hooks )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
