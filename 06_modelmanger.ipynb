{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp model.modelmanager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "#from collections.abc import Iterable,Iterator,Generator,Sequence\n",
    "#from collections import OrderedDict,defaultdict,Counter,namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader\n",
    "\n",
    "> create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from lib.data.lists import *\n",
    "from lib.model.model import *\n",
    "from lib.learner.learner import *\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class ModelManager():\n",
    "    def __init__(self,model):self.model=model\n",
    "\n",
    "    #@classmethod\n",
    "    #def create_from_model(model:nn.Module): \n",
    "\n",
    "    def find_modules(self,condition):\n",
    "        return find_submodules(self.model, condition)\n",
    "\n",
    "    def summary(self, xb:Tensor, only_leaves=True, print_mod=False):\n",
    "        #device = next(model.parameters()).device\n",
    "        #xb     = xb.to(device)\n",
    "        f      = lambda hook,mod,inp,out: print(f\"\\n{mod}\\n{out.shape}\") if print_mod else print(f\"{type(mod)} {out.shape}\")\n",
    "        mods = self.find_modules(lambda m: not isinstance(m, nn.Sequential) and not isinstance(m, ResBlock) ) if only_leaves else \\\n",
    "               self.model.children() \n",
    "        with Hooks(mods, f) as hooks: self.model(xb)\n",
    "\n",
    "    def grads_summary(self):\n",
    "        modules = self.find_modules( condition=lambda m: not isinstance(m, nn.Sequential) )\n",
    "        for module in modules:\n",
    "            if len(list(module.children()))==0:\n",
    "                requires_grad     = [p.requires_grad for p in module.parameters(recurse=False)]\n",
    "                str_requires_grad = \"None \"    \n",
    "                if len(requires_grad) > 0:    \n",
    "                    str_requires_grad = \"False\" if sum(requires_grad) == 0 else \"True \" if sum(requires_grad)==len(requires_grad) else \"None\"\n",
    "                print(f\"requires_grad: {str_requires_grad} : {type(module).__name__}\")\n",
    "\n",
    "    def save(self, path, subdir=\"models\"):\n",
    "        mdl_path = Path(path)/subdir\n",
    "        mdl_path.mkdir(exist_ok=True)\n",
    "        st = self.model.state_dict()\n",
    "        torch.save(st, mdl_path/'iw5')\n",
    "    \n",
    "    def load(self, path, subdir=\"models\"):\n",
    "        mdl_path = Path(path)/subdir\n",
    "        st = torch.load(mdl_path/'iw5')    \n",
    "        self.model.load_state_dict(st)\n",
    "\n",
    "    @staticmethod\n",
    "    def set_grad(module, requires_grad, train_bn=False):\n",
    "        if isinstance(module, (nn.BatchNorm2d)): return\n",
    "\n",
    "        for p in module.parameters(recurse=False):\n",
    "            p.requires_grad_(requires_grad)\n",
    "\n",
    "    def change_requires_grad_(self, modules, requires_grad, train_bn):\n",
    "        condition = lambda m: not isinstance(m, nn.Sequential)\n",
    "        selection = []\n",
    "        for m in modules:   selection.extend( ModelManager.find_submodules(m, condition) )\n",
    "        for m in selection: ModelManager.set_grad(m, requires_grad, train_bn)\n",
    "        \n",
    "    def freeze( self, train_bn=False ):\n",
    "        self.change_requires_grad_([self.model[0]], requires_grad=False, train_bn=train_bn)    \n",
    "        self.change_requires_grad_(self.model[1:],  requires_grad=True,  train_bn=train_bn)\n",
    "    \n",
    "    def unfreeze( self, train_bn=False ):\n",
    "        self.change_requires_grad_(self.model,    requires_grad=True, train_bn=train_bn)    \n",
    "        \n",
    "    def getFirstbatch(self, databunch:DataBunch, normalization:Callback ):\n",
    "        cbfs  = [partial(BatchTransformXCallback, tfm = normalization), GetOneBatchCallback]\n",
    "        learn = Learner( self.model, databunch, loss_func=None)\n",
    "        learn.fit(1, opt=None, cb_funcs=cbfs)\n",
    "        cb    = learn.find_subcription_by_cls(GetOneBatchCallback)\n",
    "        if cb is None: print(\"cb is None\")\n",
    "        return cb.xb, cb.yb\n",
    "    \n",
    "    def adapt_model(self, databunch, normalization):\n",
    "        #get rid of norm\n",
    "        cut   = next( i for i,o in enumerate(self.model.children()) if isinstance(o,nn.AdaptiveAvgPool2d) )\n",
    "        m_cut = self.model[:cut]\n",
    "    \n",
    "        xb,_  = self.getFirstbatch( databunch, normalization )\n",
    "        pred  = m_cut(xb)\n",
    "        ni    = pred.shape[1]\n",
    "    \n",
    "        self.model = nn.Sequential(\n",
    "            m_cut, \n",
    "            #AdaptiveConcatPool2d(), \n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            Flatten(),\n",
    "            nn.Linear(ni, databunch.c_out)\n",
    "            #nn.Linear(ni*2, data.c_out)\n",
    "        )\n",
    "        \n",
    "    def predict(self, input_data, tfm_input):\n",
    "        with torch.no_grad():\n",
    "            return self.model( tfm_input(torch.tensor(input_data) ) )\n",
    "\n",
    "class CnnModelManager(ModelManager):\n",
    "\n",
    "    def initialize(self, is_resnet:bool, uniform:bool=False, a=0.0, nonlinearity=\"relu\"):\n",
    "        if isinstance(self.model,XResNet): \n",
    "            self.model.initialize(uniform, a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\"  \n",
    "\n",
    "\n",
    "def init_cnn_resnet(m):\n",
    "    if getattr(m, 'bias', None) is not None: nn.init.constant_(m.bias, 0)\n",
    "    if isinstance(m, (nn.Conv2d,nn.Linear)): nn.init.kaiming_normal_(m.weight)\n",
    "    for l in m.children(): init_cnn_resnet(l)\n",
    "\n",
    "def bn_splitter(m):\n",
    "    def _bn_splitter(l, g1, g2):\n",
    "        if isinstance(l, nn.BatchNorm2d): g2 += l.parameters()\n",
    "        elif hasattr(l, 'weight'): g1 += l.parameters()\n",
    "        for ll in l.children(): _bn_splitter(ll, g1, g2)\n",
    "        \n",
    "    g1,g2 = [],[]\n",
    "    _bn_splitter(m[0], g1, g2)\n",
    "    \n",
    "    g2 += m[1:].parameters()\n",
    "    return g1,g2\n",
    "a,b = bn_splitter(learn.model)\n",
    "\"\"\"        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test that we can load mnist or imagenette files\n",
    "from ai_pytorch.data.external import *\n",
    "import numpy as np\n",
    "\n",
    "path   = untar_data(URLs.MNIST)\n",
    "#path  = untar_data(URLs.IMAGENETTE_160)\n",
    "files  = ImageList.from_files( path )\n",
    "print( f\"path:{path}\\nnb-files: {len(files)} Image size: {files[0].size}\" )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ibx_train      = files.label_by_func( lambda path: path.parent.parent.name==\"training\" )\n",
    "labels         = files.label_by_func( lambda path: int(path.parent.name) )\n",
    "uniques_labels = labels.unique()\n",
    "train,test     = files.split2ways(ibx_train)\n",
    "train_labels, test_labels = labels.split2ways(ibx_train)\n",
    "ds_train       = Dataset(train, train_labels )\n",
    "ds_test        = Dataset(test,  test_labels  )\n",
    "dataBunch      = DataBunch(ds_train, ds_test, c_in=1, c_out=len(uniques_labels))\n",
    "print(uniques_labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "type(files), type(train), type(test), type(train_labels), type(test_labels), len(files), len(train), len(test), len(train_labels), len(test_labels), "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test that transformation of and PIL.Image og ndarray image to tensorIUmage and vice versa \n",
    "x1 = TensorImage.from_image( files[0] )\n",
    "x2 = TensorImage.from_image( files[1] )\n",
    "\n",
    "gph = Graphics()\n",
    "gph.show_image( TensorImage.as_image(x1)/255. )\n",
    "gph.show()\n",
    "gph.show_image( TensorImage.as_image(x2)/255. )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# test that the tranformations works\n",
    "image     = TensorImage.from_image( files[1] )\n",
    "affines   = AffineTransforms([Rotation(.3,30), ShiftScale(.3,shift=0.5,scale=0.25)])\n",
    "image_tfm = affines(image)\n",
    "\n",
    "gph.show_tensorimage(image)\n",
    "gph.show()\n",
    "gph.show_tensorimage(image_tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted app_image_02_imagenette_optimizers.ipynb.\n",
      "Converted fin_01_candlestick.ipynb.\n",
      "Converted fin_02_simfin_data.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
