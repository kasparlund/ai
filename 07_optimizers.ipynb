{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp learner.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from typing import *\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import tensor\n",
    "import math\n",
    "\n",
    "from lib.data.lists import *\n",
    "from lib.learner.learner import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from functools import *\n",
    "\n",
    "def annealer(f):\n",
    "    def _inner(start, end): return partial(f, start, end)\n",
    "    return _inner\n",
    "\n",
    "@annealer\n",
    "def sched_lin(start, end, pos): return start + pos*(end-start)\n",
    "\n",
    "@annealer\n",
    "def sched_cos(start, end, pos): return start + (1 + math.cos(math.pi*(1-pos))) * (end-start) / 2\n",
    "@annealer\n",
    "def sched_no(start, end, pos):  return start\n",
    "@annealer\n",
    "def sched_exp(start, end, pos): return start * (end/start) ** pos\n",
    "\n",
    "def combine_scheds(pcts, scheds):\n",
    "    assert sum(pcts) == 1.\n",
    "    pcts = tensor([0] + pcts)\n",
    "    assert torch.all(pcts >= 0)\n",
    "    pcts = torch.cumsum(pcts, 0)\n",
    "    def _inner(pos):\n",
    "        idx = (pos >= pcts).nonzero().max()\n",
    "        actual_pos = (pos-pcts[idx]) / (pcts[idx+1]-pcts[idx])\n",
    "        return scheds[idx](actual_pos)\n",
    "    return _inner\n",
    "\n",
    "\n",
    "class OptimizerFunction():\n",
    "    \"abstract class to implement optimization of the models parameters and acces the current optimizer parameters\"\n",
    "    def __init__(self, sched_func): \n",
    "        self.sched_func, self.optimizers,  = sched_func, None\n",
    "\n",
    "    def getOptimizers(self): return self.optimizers\n",
    "    \n",
    "    def update(self, progress:float): \n",
    "        \"progress: is the percentage progres in the planned epochs and iterations\"\n",
    "        self.optimizers = self.updateOptimizers(progress)\n",
    "\n",
    "    def optimize(self, params:Collection[torch.nn.Parameter], mov_avg:torch.nn.Parameter=None): \n",
    "        \"update the parameters. mov_avg may is None, not None when using OptimizerCallback, StatefulOptimizer respectively\"\n",
    "        raise NotImplementedError(\"def optimize Must be implemented\")\n",
    "\n",
    "    def updateOptimizers(self, progress:float): \n",
    "        raise NotImplementedError(\"def getOptimizers: Must be implemented\")\n",
    "    \n",
    "class OptimizerCallback(Callback):\n",
    "    def begin_fit(self,e:Event):\n",
    "        #count iteration to adjust the training params to the progress in the training cycle\n",
    "        self.n_iter  = 0      \n",
    "        self.params  = [ p for p in e.learn.model.parameters() if p.requires_grad ]\n",
    "        #self.mov_avg = [ p*0 for p in self.params ]\n",
    "\n",
    "    def begin_batch(self,e:Event): \n",
    "        if e.learn.in_train:\n",
    "            self.fractional_cycle = min(1.,self.n_iter /(e.learn.iters * e.learn.epochs))\n",
    "            e.learn.opt.update(self.fractional_cycle)\n",
    "        \n",
    "    def begin_step(self, e:Event):\n",
    "        if e.learn.in_train:\n",
    "            #for p in self.params: e.learn.opt.optimize(p)\n",
    "            e.learn.opt.optimize(self.params)\n",
    "            #self.mom = 0.9    \n",
    "            #self.mov_avg = self.mov_avg*self.mom + (1-self.mom) *p.grad.data\n",
    "            #state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)\n",
    "           \n",
    "    def after_step(self, e:Event):            \n",
    "        if e.learn.in_train:\n",
    "            for p in self.params:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "                \n",
    "    def after_batch(self,e:Event): \n",
    "        if e.learn.in_train: self.n_iter += 1\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    def __init__(self, max_iter=100, min_lr=1e-6, max_lr=10, beta = 0.8):\n",
    "        self.max_iter,self.min_lr,self.max_lr, self.best_loss = max_iter,min_lr,max_lr, 1e9\n",
    "        self.beta = beta\n",
    "\n",
    "    def begin_fit(self, e:Event):\n",
    "        self.n_iter = 0\n",
    "        self.mov_avg = -1\n",
    "        self.losses,self.smooth_losses  = [], []\n",
    "        self.lrs  = []\n",
    "        #self.lrs  = [[] for _ in e.learn.opt.param_groups]\n",
    "        \n",
    "    def after_loss(self, e:Event):\n",
    "        if not e.learn.in_train: return\n",
    "        pos = self.n_iter/self.max_iter\n",
    "        e.learn.opt.lr = self.min_lr * (self.max_lr/self.min_lr) ** pos\n",
    "        \n",
    "    def after_batch(self, e:Event):\n",
    "        if e.learn.in_train:         \n",
    "            loss         = e.learn.loss.item()\n",
    "            self.mov_avg = loss if self.mov_avg < 0 else self.mov_avg*self.beta + loss*(1-self.beta) \n",
    "            self.losses.append(loss)\n",
    "            self.smooth_losses.append( self.mov_avg )            \n",
    "            self.lrs.append(e.learn.opt.lr)\n",
    "            self.n_iter += 1 \n",
    "            \n",
    "    def after_step(self, e:Event):\n",
    "        if not e.learn.in_train: return\n",
    "        if self.n_iter >= self.max_iter or self.mov_avg > 4.0*self.best_loss:\n",
    "            e.learn.stop = True\n",
    "        if e.learn.loss <  self.best_loss: self.best_loss = e.learn.loss.item()\n",
    "            \n",
    "    def plot_lr  (self, pgid=-1): \n",
    "        fig, ax = plt.subplots()\n",
    "        plt.plot(self.lrs, label='learning rate')        \n",
    "        ax.set(xlabel='iteration', ylabel='learning rate', title='learning rate finder')  \n",
    "        plt.legend(loc='upper left')\n",
    "        \n",
    "    def plot_loss(self, skip_start=0, skip_end=0 ):          \n",
    "        fig, ax = plt.subplots()\n",
    "        s  = slice(skip_start,-skip_end) if skip_end>0 else slice(skip_start, None)\n",
    "        l1 = plt.plot(self.lrs[s], self.losses[s], label='raw')\n",
    "        l2 = plt.plot(self.lrs[s], self.smooth_losses[s], \n",
    "                      label=f\"smoothed:{self.beta}: mov_avg*a +(1-{self.beta})*loss\")\n",
    "        plt.xscale('log')\n",
    "        ax.set(xlabel='learning rate', ylabel='losses', title='learning rate finder')  \n",
    "        plt.legend(loc='lower left')        \n",
    "\n",
    "class SGD(OptimizerFunction):\n",
    "    \"sgd with momentum and weight decay\"\n",
    "    #params = params - learning_rate * params_grad - learning_rate * wd * params\"\n",
    "    def __init__(self,sched_func, max_lr=0.3, max_wd=0.0): \n",
    "        super().__init__(sched_func)\n",
    "        self.lr, self.max_lr = max_lr, max_lr\n",
    "        self.wd, self.max_wd = max_wd, max_wd\n",
    "    def updateOptimizers(self, progress:float):\n",
    "        self.lr = self.max_lr*self.sched_func(progress)\n",
    "        self.wd = self.max_wd*self.sched_func(progress) if self.max_wd>0 else 0\n",
    "        return {\"lr\":self.lr,\"wd\":self.wd}\n",
    "    def optimize(self, params:Collection[torch.nn.Parameter], mov_avg:torch.nn.Parameter=None):\n",
    "        for p in params: \n",
    "            if self.wd > 0.: \n",
    "                p.data.add_(p.data, alpha = -self.lr*self.wd)\n",
    "            p.data.add_(p.grad.data, alpha = -self.lr)\n",
    "            \n",
    "class AverageGrad():\n",
    "    def __init__(self): self.avg = None\n",
    "    def update(self, mom, params):\n",
    "        #mean_avg, mean_grad = 0.,0.\n",
    "        if self.avg is None: \n",
    "            self.avg = [p.grad.data.clone() for p in params]\n",
    "        else:   \n",
    "            for avg,p in zip(self.avg,params) :\n",
    "                avg.mul_(mom).add_(p.grad.data,alpha = 1-mom)\n",
    "                #avg[:] = (self.mom * avg + (1 - self.mom ) * p.grad.data)[:]\n",
    "            #    mean_avg  += avg.abs().mean()\n",
    "            #    mean_grad += p.grad.data.abs().mean()\n",
    "        #print(f\"mean_abs_avg, mean_ab_p: {mean_avg:.4f}, {mean_grad:.4f}\" )\n",
    "        \n",
    "class AverageSqrGrad():\n",
    "    def __init__(self): \n",
    "        self.avg = None\n",
    "        self.sqr_mom = 0.99\n",
    "\n",
    "    def update(self, mom, params):\n",
    "        if self.avg is None: \n",
    "            #no debiase that the first avg is set to 100% of the first squared gradient\n",
    "            self.avg = [p.grad.data.pow(2) for p in params]\n",
    "        else:   \n",
    "            for avg,p in zip(self.avg,params) :\n",
    "                avg.mul_(self.sqr_mom).addcmul_(p.grad.data, p.grad.data, value=1-self.sqr_mom)\n",
    "\n",
    "class SGD_Momentum(OptimizerFunction):\n",
    "    \"sgd with momentum and weight decay\"\n",
    "    #mov_avg = momentum*mov_avg +(1-momentum) * params_grad\n",
    "    #params  = params - learning_rate * mov_avg - learning_rate * wd * params_grad\n",
    "    def __init__(self,sched_func, max_lr=0.3, moms=(0.85,0.95), max_wd=0.): \n",
    "        super().__init__(sched_func)\n",
    "        self.lr,  self.max_lr     = max_lr, max_lr\n",
    "        self.mom, self.moms_range = moms[1], moms\n",
    "        self.wd,  self.max_wd     = max_wd, max_wd\n",
    "        self.avg_grad = AverageGrad()\n",
    "    def updateOptimizers(self, progress:float):\n",
    "        self.lr  = self.max_lr*self.sched_func(progress)\n",
    "        self.mom = self.moms_range[0] + (self.moms_range[1]-self.moms_range[0])*self.sched_func(progress)\n",
    "        self.wd  = self.max_wd*self.sched_func(progress) if self.max_wd>0 else 0\n",
    "        return {\"lr\":self.lr,\"mom\":self.mom,\"wd\":self.wd}\n",
    "    def optimize(self, params:Collection[torch.nn.Parameter], mov_avg:torch.nn.Parameter=None):\n",
    "        \n",
    "        self.avg_grad.update(self.mom,params)\n",
    "                \n",
    "        for mov_avg,p in zip(self.avg_grad.avg,params) :\n",
    "            if self.wd > 0.: p.data.mul_(1-self.lr*self.wd)\n",
    "            p.data.add_(mov_avg, alpha=-self.lr)\n",
    "\n",
    "class Adam(OptimizerFunction):\n",
    "    #wd as in Decoupled Weight Decay Regularization: https://arxiv.org/abs/1711.05101\n",
    "    #momentum_1, momentum_2, eps are typically 0.9, 0.99, 1e-8\n",
    "    #avg     = momentum_1*avg     +(1-momentum_1) * params_grad\n",
    "    #sqr_avg = momentum_2*avg +(1-momentum_2) * params_grad * params_grad\n",
    "    #params  = params - learning_rate * avg / (sqrt(sqr_avg)+eps)\n",
    "    def __init__(self,sched_func, max_lr=0.3, moms=(0.95,0.85), max_wd=0.): \n",
    "        super().__init__(sched_func)\n",
    "        self.lr,  self.max_lr     = max_lr, max_lr\n",
    "        self.mom, self.moms_range = moms[1], moms\n",
    "        self.wd,  self.max_wd     = max_wd, max_wd\n",
    "        self.eps = 1e-8\n",
    "        self.avg_grad     = AverageGrad()\n",
    "        self.avg_sqr_grad = AverageSqrGrad()\n",
    "    def updateOptimizers(self, progress:float):\n",
    "        self.lr  = self.max_lr*self.sched_func(progress)\n",
    "        self.mom = self.moms_range[0] + (self.moms_range[1]-self.moms_range[0])*self.sched_func(progress)\n",
    "        self.wd  = self.max_wd\n",
    "        #self.wd  = self.max_wd*(1-min(1,self.sched_func(progress))) if self.max_wd>0 else 0\n",
    "        return {\"lr\":self.lr,\"mom\":self.mom,\"wd\":self.wd}\n",
    "    def optimize(self, params:Collection[torch.nn.Parameter], avg:torch.nn.Parameter=None):\n",
    "        self.avg_grad.update(self.mom,params)\n",
    "        self.avg_sqr_grad.update(self.mom,params)\n",
    "                \n",
    "        for p,avg_grad,avg_sqr_grad in zip(params,self.avg_grad.avg,self.avg_sqr_grad.avg) :\n",
    "            #p.data.add_(-self.lr, avg_grad/(avg_sqr_grad.sqrt()+self.eps))\n",
    "            if self.wd > 0.: p.data.mul_(1-self.lr*self.wd)\n",
    "            p.data.addcdiv_(avg_grad, avg_sqr_grad.sqrt().add_(self.eps), value = -self.lr )\n",
    "                        \n",
    "\n",
    "class LAMB(OptimizerFunction):\n",
    "    #momentum_1, momentum_2, eps are typically 0.9, 0.99, 1e-8\n",
    "    #avg     = momentum_1*avg     +(1-momentum_1) * params_grad\n",
    "    #sqr_avg = momentum_2*avg +(1-momentum_2) * params_grad * params_grad\n",
    "    #params  = params - learning_rate * avg / (sqrt(sqr_avg)+eps)\n",
    "    def __init__(self,sched_func, max_lr=0.3, moms=(0.85,0.95), max_wd=0.): \n",
    "        super().__init__(sched_func)\n",
    "        self.lr,  self.max_lr     = max_lr, max_lr\n",
    "        self.mom, self.moms_range = 0.5*(moms[0]+moms[1]), moms\n",
    "        self.wd,  self.max_wd     = max_wd, max_wd\n",
    "        self.eps = 1e-9\n",
    "        self.avg_grad     = AverageGrad()\n",
    "        self.avg_sqr_grad = AverageSqrGrad()\n",
    "    def updateOptimizers(self, progress:float):\n",
    "        #self.lr  = self.max_lr*self.sched_func(progress)\n",
    "        #self.mom = self.moms_range[0] + (self.moms_range[1]-self.moms_range[0])*self.sched_func(progress)\n",
    "        #self.wd  = self.max_wd\n",
    "        #self.wd  = self.max_wd*(1-min(1,self.sched_func(progress))) if self.max_wd>0 else 0\n",
    "        return {\"lr\":self.lr,\"mom\":self.mom,\"wd\":self.wd}\n",
    "    def optimize(self, params:Collection[torch.nn.Parameter], avg:torch.nn.Parameter=None):\n",
    "        self.avg_grad.update(self.mom,params)\n",
    "        self.avg_sqr_grad.update(self.mom,params)\n",
    "\n",
    "        for p,avg_grad,avg_sqr_grad in zip(params,self.avg_grad.avg,self.avg_sqr_grad.avg) :\n",
    "            r1   = p.data.pow(2).mean().sqrt() #magnitude of parameters: square-root of mean squared parameters\n",
    "            \n",
    "            step = avg_grad /(avg_sqr_grad.sqrt()+self.eps)\n",
    "            if self.wd>0 : step.add_(p.data, alpha = self.wd)\n",
    "                \n",
    "            r2 = step.pow(2).mean().sqrt()     #magnitude of gradients: square-root of mean squared gradients\n",
    "            \n",
    "            #p.data.add_(-self.lr * min(r1/(r2+self.eps),10), step)\n",
    "            p.data.add_(step, alpha = -self.lr * r1/(r2+self.eps))\n",
    "\n",
    "## Mixup \n",
    "#[mixup article](https://arxiv.org/abs/1710.09412) propose to train the model on a mix of the pictures of the training set. Instead of feeding the model the raw images, we take two (which could be in the same class or not) and do a linear combination of them: in terms of tensor it's\n",
    "#``` python\n",
    "#new_image = t * image1 + (1-t) * image2\n",
    "#```\n",
    "#where t is a float between 0 and 1. Assuming your targets are one-hot encoded, then the target we assign to that image is the same combination of the original targets:\n",
    "#``` python\n",
    "#new_target = t * target1 + (1-t) * target2\n",
    "\n",
    "from torch.distributions.beta import Beta\n",
    "\n",
    "class NoneReduce():\n",
    "    def __init__(self, loss_func): \n",
    "        self.loss_func,self.old_red = loss_func,None\n",
    "        \n",
    "    def __enter__(self):\n",
    "        if hasattr(self.loss_func, 'reduction'):\n",
    "            self.old_red = getattr(self.loss_func, 'reduction')\n",
    "            setattr(self.loss_func, 'reduction', 'none')\n",
    "            return self.loss_func\n",
    "        else: return partial(self.loss_func, reduction='none')\n",
    "        \n",
    "    def __exit__(self, type, value, traceback):\n",
    "        if self.old_red is not None: setattr(self.loss_func, 'reduction', self.old_red)    \n",
    "\n",
    "def unsqueeze(input, dims):\n",
    "    for dim in listify(dims): input = torch.unsqueeze(input, dim)\n",
    "    return input\n",
    "\n",
    "def reduce_loss(loss, reduction='mean'):\n",
    "    return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss    \n",
    "\n",
    "def lerp(v1, v2, beta): return beta*v1 + (1-beta)*v2\n",
    "\n",
    "\n",
    "class MixUp(Callback):\n",
    "    #_order = 90 #Runs after normalization and cuda\n",
    "    #should introduce before_transform, after_transform and change  mixup.begin_batch to after_transform\n",
    "    #alternatively make group of callbacks to control order\n",
    "    def __init__(self, α:float=0.4): self.distrib = Beta(tensor([α]), tensor([α]))\n",
    "    \n",
    "    def begin_fit(self,e:Event): \n",
    "        self.old_loss_func,e.learn.loss_func = e.learn.loss_func,self.loss_func\n",
    "        self.learn = e.learn\n",
    "    \n",
    "    def after_preprocessing(self,e:Event):\n",
    "        if not e.learn.in_train: return  #Only mixup things during training\n",
    "        λ = self.distrib.sample( (e.learn.yb.size(0),) ).squeeze().to(e.learn.xb.device)\n",
    "        λ = torch.stack([λ, 1-λ], 1)\n",
    "        self.λ  = unsqueeze(λ.max(1)[0], [1,2,3])\n",
    "        shuffle = torch.randperm(e.learn.yb.size(0)).to(e.learn.xb.device)\n",
    "        xb1,self.yb1 = e.learn.xb[shuffle],e.learn.yb[shuffle]\n",
    "        e.learn.xb   = lerp(e.learn.xb, xb1, self.λ)\n",
    "        \n",
    "    def after_fit(self,e:Event): e.learn.loss_func = self.old_loss_func\n",
    "    \n",
    "    def loss_func(self, pred, yb):\n",
    "        if not self.learn.in_train: return self.old_loss_func(pred, yb)\n",
    "        with NoneReduce(self.old_loss_func) as loss_func:\n",
    "            loss1 = loss_func(pred, yb)\n",
    "            loss2 = loss_func(pred, self.yb1)\n",
    "        loss = lerp(loss1, loss2, self.λ)\n",
    "        return reduce_loss(loss, getattr(self.old_loss_func, 'reduction', 'mean'))\n",
    "\n",
    "\n",
    "#\"\"\"\n",
    "class ParamScheduler(Callback):\n",
    "    def __init__(self, pname, sched_func): self.pname, self.sched_func = pname,sched_func\n",
    "\n",
    "    def begin_fit(self,e:Event):\n",
    "        #count iteration to adjust the training params to the progress in the training cycle\n",
    "        self.n_iter = 0\n",
    "            \n",
    "    def begin_batch(self,e:Event): \n",
    "        if e.learn.in_train: \n",
    "            for h in e.learn.opt.hypers:\n",
    "                fractional_cycle = min(1.,self.n_iter /(e.learn.iters * e.learn.epochs))\n",
    "                h[self.pname] = self.sched_func(fractional_cycle)\n",
    "\n",
    "    def after_batch(self,e:Event): \n",
    "        if e.learn.in_train: self.n_iter += 1\n",
    "#\"\"\"\n",
    "\n",
    "\n",
    "########################## Label smoothing ########################\n",
    "\"\"\"\n",
    "Another regularization technique that's often used is label smoothing. \n",
    "It's designed to make the model a little bit less certain of it's decision by changing a little bit its target: \n",
    "instead of wanting to predict 1 for the correct class and 0 for all the others, we ask it to predict `1-ε` \n",
    "for the correct class and `ε` for all the others, with `ε` a (small) positive number and N the number of classes.\n",
    "This can be written as:\n",
    "\n",
    "$$loss = (1-ε) ce(i) + ε \\sum ce(j) / N$$\n",
    "\n",
    "where `ce(x)` is cross-entropy of `x` (i.e. $-\\log(p_{x})$), and `i` is the correct class. This can be coded in a loss function:\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, ε:float=0.1, reduction='mean'):\n",
    "        super().__init__()\n",
    "        self.ε,self.reduction = ε,reduction\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
    "        nll  = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
    "        return lerp(loss/c, nll, self.ε)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#export\n",
    "\n",
    "import operator\n",
    "def test(a,b,cmp,cname=None):\n",
    "    if cname is None: cname=cmp.__name__\n",
    "    assert cmp(a,b),f\"{cname}:\\n{a}\\n{b}\"\n",
    "def test_eq(a,b): test(a,b,operator.eq,'==')\n",
    "def test_ne(a,b): test(a,b,operator.ne,'!=')\n",
    "def near(a,b): return torch.allclose(a, b, rtol=1e-3, atol=1e-5)\n",
    "def test_near(a,b): test(a,b,near)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 04_databunchs_undone.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted augmentation_cpu.ipynb.\n",
      "Converted data_block.ipynb.\n",
      "Converted imagenette_optimizers.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted mnist_experiments.ipynb.\n",
      "Converted mnist_initi_batchnorm.ipynb.\n",
      "Converted transfer_learning.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
