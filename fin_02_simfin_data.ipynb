{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87513f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp finance.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b50821",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c8a9a1b",
   "metadata": {},
   "source": [
    "#add the parent directiry so that wecan access modules the and inits subdirectories\n",
    "import sys, os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir  = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from lib.data.lists import *\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the main functionality from the SimFin Python API.\n",
    "import simfin as sf\n",
    "\n",
    "# Import names used for easy access to SimFin's data-columns.\n",
    "from simfin.names import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc0b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version of the SimFin Python API.\n",
    "sf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e732be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def initiateSimFin(key='free'):\n",
    "    # SimFin data-directory.\n",
    "    sf.set_data_dir('~/simfin_data/')\n",
    "    # SimFin load API key or use free data.\n",
    "    sf.load_api_key(path='~/simfin_api_key.txt', default_key=key)\n",
    "    return Path.home()/\"simfin_data\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#   extrem_increase,extrem__decrease = [], 2.5, -.73\n",
    "#   extrem_increase,extrem__decrease = [], 1.5, -.5\n",
    "EXTREM_HL = \"extrem_hl\"\n",
    "def flagstocks_extrem_hl(df_prices, extrem_increase, extrem__decrease):\n",
    "    #remove mature stocks with \"nan\" prices\n",
    "    if EXTREM_HL in df_prices.columns:\n",
    "        df_prices.drop(labels=EXTREM_HL,axis=1,inplace=True)\n",
    "\n",
    "    gt_p = df_prices[[HIGH]].gt(extrem_increase)\n",
    "    gt_p.insert(0,\"lt\",df_prices[[LOW]].lt(extrem__decrease))\n",
    "    df_prices.insert(df_prices.columns.get_loc(SIMFIN_ID)+1,EXTREM_HL,gt_p.any(axis=1))\n",
    "    return df_prices\n",
    "\n",
    "TOO_FEW_DAYS = \"too_few_days\"\n",
    "def flagstocks_too_few_trading_days(df_prices, minimum_tradingdays):\n",
    "    grp_res = df_prices.groupby([\"Ticker\"]).apply(lambda group: len(group) < minimum_tradingdays)\n",
    "    grp_res.name = TOO_FEW_DAYS\n",
    "    merged = df_prices[[SIMFIN_ID]].join(grp_res, on=TICKER)\n",
    "    if TOO_FEW_DAYS in df_prices.columns:\n",
    "        df_prices.drop(labels=TOO_FEW_DAYS,axis=1,inplace=True)\n",
    "    df_prices.insert(df_prices.columns.get_loc(SIMFIN_ID)+1, TOO_FEW_DAYS, merged[TOO_FEW_DAYS].values)\n",
    "    return df_prices\n",
    "\n",
    "HAS_NAN_DAYS = \"has_nan_days\"\n",
    "def flagstocks_with_nan_days(df_prices):\n",
    "    grp_res = df_prices.groupby([\"Ticker\"]).apply(lambda group: group[[OPEN,LOW,HIGH,CLOSE]].isnull().values.any() )\n",
    "    grp_res.name = HAS_NAN_DAYS\n",
    "    merged = df_prices[[SIMFIN_ID]].join(grp_res, on=TICKER)\n",
    "    if HAS_NAN_DAYS in df_prices.columns:\n",
    "        df_prices.drop(labels=HAS_NAN_DAYS,axis=1,inplace=True)\n",
    "    df_prices.insert(df_prices.columns.get_loc(SIMFIN_ID)+1, HAS_NAN_DAYS, merged[HAS_NAN_DAYS].values)\n",
    "    return df_prices\n",
    "\n",
    "def findInValidStocks(df_prices):\n",
    "    flags  = [HAS_NAN_DAYS,TOO_FEW_DAYS,EXTREM_HL]\n",
    "    groups = df_prices.groupby([\"Ticker\"])\n",
    "    return np.array([name for (name,group) in groups if group[flags].any(axis=1).any()])\n",
    "\n",
    "def findValidStocks(df_prices):\n",
    "    flags  = [HAS_NAN_DAYS,TOO_FEW_DAYS,EXTREM_HL]\n",
    "    groups = df_prices.groupby([\"Ticker\"])\n",
    "    return np.array([name for (name,group) in groups if not group[flags].any(axis=1).any()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fead46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from lib.data.lists import *\n",
    "import torch\n",
    "from torch.tensor import *\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "class Data():\n",
    "    def __init__(self, df_prices):\n",
    "        self.df_prices = df_prices\n",
    "        self.np_prices = None\n",
    "        self.seq_len   = None\n",
    "        self.columns   = None  #training and test columns\n",
    "        #insert a row counter/index\n",
    "        #if \"idx\" not in self.df_prices.columns:\n",
    "        #    self.df_prices.insert(loc=0, column=\"idx\", value=np.arange(len(self.df_prices),dtype=int))\n",
    "        self.df_prices[\"idx\"] = np.arange(len(self.df_prices),dtype=int)\n",
    "\n",
    "    #set sequence length and columns used for training and test\n",
    "    def changeTraining(self,seq_len,columns,n_predict_columns):\n",
    "        self.seq_len = seq_len\n",
    "        self.columns = columns\n",
    "        self.n_predict_columns = n_predict_columns\n",
    "        \n",
    "        #point to the first predictabel sample in a stock. \n",
    "        #this sample is predicted using df_prices[:seq_len,columns]\n",
    "        ix_predict = np.arange(len(self.df_prices),dtype=int)        \n",
    "        for g,d in self.df_prices.groupby([\"Ticker\"]):\n",
    "            ix_predict[d.idx[:seq_len]] = -1\n",
    "        self.df_prices[\"ix_predict\"] = ix_predict\n",
    "        self.np_prices = self.df_prices[self.columns].to_numpy()\n",
    "\n",
    "    def getTensor(self, ix, r_mixin=0.0, r_ix=None):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "\n",
    "        prices = self.np_prices #readability and local pointer is faster if used multiple times\n",
    "        pi     = prices[ix - self.seq_len : ix ] \n",
    "        pt     = prices[ix, :self.n_predict_columns ]   \n",
    "        if r_ix:\n",
    "            #mix with another sequence\n",
    "            ro  = 1 - r_mixin\n",
    "            pi  = pi*ro + r_mixin*prices[r_ix - self.seq_len : r_ix]\n",
    "            pt  = pt*ro + r_mixin*prices[r_ix,:self.n_predict_columns]\n",
    "        return torch.tensor( pi ), torch.tensor( pt ) #, index\n",
    "    \"\"\"\n",
    "    def getTensor(self, ix, r_mixin=0.0, ix_r=None):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "\n",
    "        prices = self.np_prices #readability and local pointer is faster if used multiple times\n",
    "        ixb    = ix - self.seq_len\n",
    "        ixe    = ix+len(self.columns)\n",
    "        pi     = torch.tensor( prices[ixb:ix ] )\n",
    "        pt     = torch.tensor( prices[ ix,:]   )\n",
    "        if r_mixin > 0.0:\n",
    "            #mix with another sequence\n",
    "            ix  = ix_r\n",
    "            ixb = ix - self.seq_len\n",
    "            ixe = ix + len(self.columns)\n",
    "            ro  = 1 - r_mixin\n",
    "            #print(f\"ixb,ix,ixe: {ixb,ix,ixe}\")\n",
    "            pi  = pi.mul_(ro) + torch.tensor( prices[ixb:ix] ).mul_(r_mixin)\n",
    "            pt  = pt.mul_(ro) + torch.tensor( prices[ix,:]   ).mul_(r_mixin)\n",
    "            #pi  = pi*ro + torch.tensor( prices[ixb:ix] )*r_mixin\n",
    "            #pt  = pt*ro + torch.tensor( prices[ix,:]   )*r_mixin\n",
    "            #pi  = pi.mul_(ro).add_( torch.tensor( prices[ixb:ix] ).mul_(r_mixin) )\n",
    "            #pt  = pt.mul_(ro).add_( torch.tensor( prices[ix,:]   ).mul_(r_mixin) )\n",
    "        return pi, pt #, index\n",
    "        \"\"\"\n",
    "    def getSequenceIndex(self,stocks):\n",
    "        #idx_seq = self.df_prices.loc[stocks,\"ix_predict\"].to_numpy()\n",
    "        idx_seq = self.df_prices.ix_predict.loc[stocks].to_numpy()\n",
    "        idx_seq = idx_seq[idx_seq>=0]\n",
    "        return idx_seq\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb704382",
   "metadata": {},
   "source": [
    "df_prices = loadShareprices()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dad18109",
   "metadata": {},
   "source": [
    "data     = Data(df_prices)\n",
    "seq_len  = 3\n",
    "columns  = [CLOSE,OPEN]\n",
    "data.changeTraining(seq_len,columns,n_predict_columns=2)\n",
    "print(data.df_prices[:6])\n",
    "%time d = data.getTensor(ix=3)#, r_mixin=0.5, r_ix=4)\n",
    "%timeit -r 10 d = data.getTensor(ix=3)#, r_mixin=0.5, r_ix=4)\n",
    "d"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1c402a9",
   "metadata": {},
   "source": [
    "        \n",
    "    \"\"\"\n",
    "    def getTensor(self, ix, r_mixin=0.0, ix_r=None):\n",
    "        #Notice that the dataset cannot be used for a RNN where the data should been returned from a sliding window\n",
    "\n",
    "        #Calculate the start and end of all sequences so that the dataloader can load and shuffle efficiently.\n",
    "        #add an index column to df_prices that we can used to calcualte start and end of sequences\n",
    "        if not \"idx\" in self.data.df_prices.columns:\n",
    "            self.data.df_prices.insert(loc=0, column=\"idx\", value=np.arange(len(self.df_prices),dtype=int))\n",
    "\n",
    "        #create one array \"prices with interleaved values from training.columns iinterleaved in\n",
    "        #fx if the require columns are \"open\", \"close\". Then prices contain row, columns values from\n",
    "        #df_price in the following order : open_row1, close_row1, open_row2, close_row2, ...\n",
    "        self.np_prices = np.empty(self.df_prices.shape[0]*len(self.column_names))\n",
    "        for (i,c) in enumerate(self.column_names):\n",
    "            self.np_prices[i::len(self.column_names)] = self.df_prices[c].values\n",
    "\n",
    "        self.idx_seq    = self.getSequenceIndexing(self.stocks)\n",
    "    \"\"\"\n",
    "    \"\"\"    \n",
    "    def getSequenceIndexing(self):\n",
    "        #get an index to all valid samples in self.prices\n",
    "        idx_begin = [] #list of valid dataset index into the dataframe\n",
    "        stock_grps   = self.df_prices.groupby([\"Ticker\"])\n",
    "        for stock in stocks:\n",
    "            group = self.stock_grps.get_group(stock)\n",
    "            idx_begin.extend( group[\"idx\"].values[self.seq_length:len(group)] )\n",
    "        idx_begin    = np.array(idx_begin,dtype=int)\n",
    "        idx_seq      = np.empty( (len(idx_begin),2), dtype=int)\n",
    "        #idx_seq[:,0] = (idx_begin+self.seq_length)*len(self.column_names)\n",
    "        idx_seq[:,0] = idx_begin*len(self.column_names)\n",
    "        idx_seq[:,1] = idx_seq[:,0] - self.seq_length*len(self.column_names)\n",
    "        return idx_seq\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "        #get an index to all valid samples in self.prices\n",
    "        idx_begin = [] #list of valid dataset index into the dataframe\n",
    "        stock_grps   = self.df_prices.groupby([\"Ticker\"])\n",
    "        for stock in stocks:\n",
    "            group = self.stock_grps.get_group(stock)\n",
    "            idx_begin.extend( group[\"idx\"].values[self.seq_length:len(group)] )\n",
    "        idx_begin    = np.array(idx_begin,dtype=int)\n",
    "        idx_seq      = np.empty( (len(idx_begin),2), dtype=int)\n",
    "        #idx_seq[:,0] = (idx_begin+self.seq_length)*len(self.column_names)\n",
    "        idx_seq[:,0] = idx_begin*len(self.column_names)\n",
    "        idx_seq[:,1] = idx_seq[:,0] - self.seq_length*len(self.column_names)\n",
    "        return idx_seq\n",
    "        \n",
    "        \n",
    "        #get an index to all valid samples\n",
    "        idx_begin = [] #list of valid dataset index into the dataframe\n",
    "        for stock in self.stocks:\n",
    "            group = self.stock_grps.get_group(stock)\n",
    "            #idx_begin.extend( group[\"idx\"].values[:len(group)-self.seq_length] )\n",
    "            idx_begin.extend( group[\"idx\"].values[self.seq_length:len(group)] )\n",
    "        self.idx_seq = np.empty( (len(idx_begin),2), dtype=int)\n",
    "        self.idx_seq[:,0] = idx_begin\n",
    "        self.idx_seq[:,1] = self.idx_seq[:,0]-self.seq_length\n",
    "    \"\"\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d9422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#Create a dataset that uses all the stocks and daily prices\n",
    "#In order to save memory the df_prices is the same for both training and validation.\n",
    "#np_prices contains interleaved prices of the columns used for training/validation\n",
    "#fx if the require columns are \"open\", \"close\". Then np_prices contain row, columns values from\n",
    "#df_price in the following order : open_row1, close_row1, open_row2, close_row2, ...\n",
    "\n",
    "#for now self.np_prices are dublicated. Thus using more memory than needed.\n",
    "#Test and validation contain different stock taken drom the same df_prices\n",
    "\n",
    "#stocks    : are the stock tickers\n",
    "#stock_days: is the number of days pr stock. The number of days is in different for each stock\n",
    "#            The total number of samples is stock_days.sum()\n",
    "#batch_size: is the number of stocks in a batch\n",
    "#seq_length: is the number of stocks days in a batch.\n",
    "#the number of sample in the batch is seq_length*len(column_names)\n",
    "\n",
    "#As for now the dataset progress sequentially through the data during an epoch from a random offset.\n",
    "#Notice that there is no end of sequence-token to reset the model when the sequencen wraps around\n",
    "TRAIN_TEST   = \"train_test\"\n",
    "\n",
    "        \n",
    "class OHLCDataset(torch.utils.data.Dataset):\n",
    "    #x, y significes input vs output\n",
    "    def __init__(self, data, stocks, train_test=\"invalid\"):\n",
    "        self.data         = data #share data\n",
    "        self.stocks       = stocks\n",
    "\n",
    "        #Index to all start of sequences in the dataset\n",
    "        #The target is taken from the day after the sequence\n",
    "        self.idx_seq      = None\n",
    "        self.mixin        = 0.0  #ration to mix to sequences\n",
    "        self.train_test   = train_test        \n",
    "        self.idx_seq      = data.getSequenceIndex(stocks)\n",
    "        \n",
    "        \n",
    "    def changeAugmentation( self, mixing=0.5 ):\n",
    "        self.mixin = mixing\n",
    "        self.do_mix = self.mixin>0.0\n",
    "        self.ix_mix_queue = -1   \n",
    "        #self.idx_seq[random.randint(0, len(self)-1)] if self.mixin>0 else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_seq)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "        ix = self.idx_seq[index]\n",
    "\n",
    "        #mix with another sequence\n",
    "        \n",
    "        \"\"\"\n",
    "        if self.do_mix:\n",
    "            #performance optimization. It is much faste to generate many random numbers\n",
    "            #and make som book keeping \n",
    "            if self.ix_mix_queue==-1:\n",
    "                rng = np.random.default_rng()\n",
    "                self.rand_idx_seq   = rng.integers(0, len(self.idx_seq)-1,size=len(self.idx_seq))\n",
    "                #print(f\"self.rand_idx_seq.shape :{self.rand_idx_seq.shape}\\n{self.rand_idx_seq[:10]}\")\n",
    "                self.rand_fractions = rng.random(size=len(self.idx_seq))\n",
    "                #print(f\"self.rand_fractions.shape :{self.rand_fractions.shape}\\n{self.rand_fractions[:10]}\")\n",
    "                self.ix_mix_queue   = len(self.rand_idx_seq)-1\n",
    "\n",
    "            r_ix    = self.idx_seq[self.rand_idx_seq[self.ix_mix_queue]] # self.idx_seq[random.randint(0, len(self)-1)] if self.mixin>0 else None\n",
    "            r_mixin = self.rand_fractions[self.ix_mix_queue] #random.uniform(0, self.mixin )               if self.mixin>0 else 0.0\n",
    "            self.ix_mix_queue -= 1 \n",
    "            #print(f\"ix_mix_queue, r_ix,r_mixin÷{self.ix_mix_queue,r_ix,r_mixin}\")\n",
    "        else:\n",
    "            r_mixin=0.0\n",
    "            r_ix=None\n",
    "        \"\"\"\n",
    "            \n",
    "        r_ix    = self.idx_seq[random.randint(0, len(self)-1)] if self.do_mix else None\n",
    "        r_mixin = random.uniform(0, self.mixin )               if self.do_mix else 0.0\n",
    "            \n",
    "        return self.data.getTensor(ix=ix, r_mixin=r_mixin, r_ix=r_ix)\n",
    "\n",
    "    #stocks: stocks that you want the price_input and target for\n",
    "    def getBatch(self, stocks):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "        #extract the rows in df_proces containing the stocks in the batch\n",
    "        idx = self.getSequenceIndexing(stocks);\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        df_prices_selected = self.df_prices[ self.df_prices.index.get_level_values(0).isin(stocks) ]\n",
    "        stock_grps         = df_prices_selected.groupby([\"Ticker\"])\n",
    "        idx, seq_len = [], self.seq_length\n",
    "        for name,group in stock_grps:\n",
    "            idx.extend(group[\"idx\"].values[seq_len:len(group)])\n",
    "        idx = np.array(idx,dtype=int)\n",
    "\n",
    "        pt = self.df_prices.iloc[idx]\n",
    "        pi = [self.df_prices.iloc[ib:ie, self.ix_columns].values for ib,ie in zip(pt.idx - seq_len, pt.idx)]\n",
    "        return np.array(pi), self.df_prices.iloc[idx].copy()\n",
    "        \"\"\"\n",
    "\n",
    "    def dataloader(self, batch_size:int, shuffle:bool, num_workers:int=0, drop_last=False):\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size, shuffle=shuffle,\n",
    "                                           num_workers=num_workers, drop_last=drop_last)\n",
    "\n",
    "    def split2train_test(self, test_percentage):\n",
    "        #split the stocks in train and test stocks\n",
    "        #It might be better to make a split in historical vs new prices\n",
    "        ix_all   = np.arange(len(self.stocks),dtype=int)\n",
    "        np.random.shuffle(ix_all)\n",
    "\n",
    "        nb_test  = int(round(test_percentage*len(self.stocks)))\n",
    "        ix_train = ix_all[nb_test:]\n",
    "        ix_test  = ix_all[:nb_test]\n",
    "        train_stocks = self.stocks[ix_train]\n",
    "        test_stocks  = self.stocks[ix_test]\n",
    "        self.data.df_prices[TRAIN_TEST]   = \"invalid\"\n",
    "        self.data.df_prices.loc[train_stocks,TRAIN_TEST] = \"train\"\n",
    "        self.data.df_prices.loc[test_stocks,TRAIN_TEST ] = \"test\"\n",
    "        return self.__class__(self.data, self.stocks[ix_train], \"train\"),\\\n",
    "               self.__class__(self.data, self.stocks[ix_test],  \"test\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "49255ef9",
   "metadata": {},
   "source": [
    "#from numpy.random import default_rng\n",
    "len_idx =10\n",
    "rng = np.random.default_rng()\n",
    "ri=rand_idx_sequence = rng.integers(0, len_idx,size=len_idx)\n",
    "rf=rand_frations = rng.random(len_idx)\n",
    "print(ri)\n",
    "print(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "def loadShareprices():\n",
    "    #Load shareprices for testing and development\n",
    "    dataPath = initiateSimFin(key='free')\n",
    "    print(f\"dataPath:{dataPath} exists:{dataPath.exists()}\")\n",
    "\n",
    "    # Data for USA.\n",
    "    market = 'us'\n",
    "    # Daily Share-Prices.\n",
    "    return sf.load_shareprices(variant='daily', market=market)\n",
    "\n",
    "\n",
    "PREV_CLOSE = \"previous_close\"\n",
    "def alignPreviousClose(df_prices):\n",
    "    # Make a new column to show the closing price from the previous day on the same line as current day.\n",
    "    # This is done using the shift function for the dataserie\n",
    "    #\n",
    "    # Result: All stock and prices are listed in the same tabel. Therefore, the firat priceline of each stock vil now\n",
    "    # contain the close of the previous stock. For the first stock this value vil be \"nan\".\n",
    "    # These incoherent pricelines are removed in the nest step\n",
    "\n",
    "    #if PREV_CLOSE not in df_prices.columns:\n",
    "    if not PREV_CLOSE in df_prices.columns:\n",
    "        df_prices.insert(df_prices.columns.get_loc(OPEN), PREV_CLOSE, df_prices[CLOSE].shift(), allow_duplicates=False)\n",
    "\n",
    "    #identify the first stock in each stockgroup and the remove it\n",
    "    stock_name    = df_prices.index.get_level_values(0)\n",
    "    new_stock     = np.ones(len(df_prices), dtype=bool)\n",
    "    new_stock[1:] = stock_name[0:len(stock_name)-1] != stock_name[1:len(stock_name)]\n",
    "    df_prices.drop(df_prices.index[new_stock], axis=0, inplace=True)\n",
    "    return df_prices\n",
    "\n",
    "#extrem_increase,extrem__decrease = 2.5, -.73\n",
    "#extrem_increase,extrem__decrease = 1.5, -.5\n",
    "def procesSharePrices(df_prices, minimum_tradingdays=180, extrem_increase = 0.5, extrem__decrease = -0.5):\n",
    "\n",
    "    # load and identify valid data\n",
    "    df_prices = alignPreviousClose(df_prices)\n",
    "    df_prices = logMinusPreviousClose(df_prices)\n",
    "\n",
    "    flagstocks_extrem_hl(df_prices, extrem_increase, extrem__decrease)\n",
    "    flagstocks_too_few_trading_days(df_prices,minimum_tradingdays)\n",
    "    flagstocks_with_nan_days(df_prices)\n",
    "\n",
    "    validStocks, inValidStocks = findValidStocks(df_prices), findInValidStocks(df_prices)\n",
    "\n",
    "    stock_grps = df_prices.groupby([\"Ticker\"])\n",
    "    stocks     = np.array(list(stock_grps.groups))\n",
    "    sizes      = stock_grps.size()\n",
    "    print(f\"number of stocks:         {len(stocks)}\")\n",
    "    print(f\"number of valid stocks:   {len(validStocks)}\")\n",
    "    print(f\"number of invalid stocks: {len(inValidStocks)}\")\n",
    "\n",
    "    print(f\"smallest pricelines pr stock: {sizes.sort_values()[:5]}\")\n",
    "    print(f\"longest pricelines pr stock:  {sizes.sort_values()[-5:]}\")\n",
    "\n",
    "    return df_prices, stocks, validStocks, inValidStocks\n",
    "\n",
    "predict_prefix=\"predict_\"\n",
    "PREDICT_OPEN  = predict_prefix+OPEN\n",
    "PREDICT_HIGH  = predict_prefix+HIGH\n",
    "PREDICT_LOW   = predict_prefix+LOW\n",
    "PREDICT_CLOSE = predict_prefix+CLOSE\n",
    "\n",
    "def predict_stocks(dataset, modelmanager, stocks, tfm_input ):\n",
    "    price_sequences, price_targets = dataset.getBatch(stocks=stocks)\n",
    "    predictions    = modelmanager.predict(price_sequences, tfm_input)\n",
    "\n",
    "    # inser the preduction in price_targets\n",
    "    prediction_columns = [ predict_prefix + name for name in dataset.column_names ]\n",
    "    print(prediction_columns)\n",
    "    for idx,name in enumerate(prediction_columns):\n",
    "        print(idx, name)\n",
    "        if name in price_targets.columns :\n",
    "            price_targets.drop([name], axis='columns', inplace=True)\n",
    "        else:\n",
    "            price_targets.insert(len(price_targets.columns), name, predictions[:,idx-1].numpy())\n",
    "    return price_targets, prediction_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e87b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#Convert OHLC to percentages of the previous days closing price\n",
    "#tahe the log onallprice action and subtract Previousclose from from price\n",
    "#to arrive at log percentage change relativ til previous close\n",
    "def logMinusPreviousClose(df_prices):\n",
    "    df_prices[[PREV_CLOSE,OPEN,CLOSE,LOW,HIGH]] = df_prices[[PREV_CLOSE,OPEN,CLOSE,LOW,HIGH]].apply(np.log)\n",
    "    df_prices[[OPEN,CLOSE,LOW,HIGH]]            = df_prices[[OPEN,CLOSE,LOW,HIGH]].sub(df_prices[PREV_CLOSE],axis=0)\n",
    "    return df_prices\n",
    "\n",
    "normalized_suffix=\"_normalized\"\n",
    "OPEN_NORM  = OPEN +normalized_suffix\n",
    "HIGH_NORM  = HIGH +normalized_suffix\n",
    "LOW_NORM   = LOW  +normalized_suffix\n",
    "CLOSE_NORM = CLOSE+normalized_suffix\n",
    "\n",
    "def normalizeData( df_prices, stats, normalized_suffix=normalized_suffix):\n",
    "    normalized_columns = []\n",
    "    for c in [OPEN,HIGH,LOW,CLOSE]:\n",
    "        mean,std = [stats.loc[\"mean\",c], stats.loc[\"std\",c]]\n",
    "        normalized_columns.append( c+normalized_suffix )\n",
    "        df_prices[c+normalized_suffix] = df_prices[c].div(std)\n",
    "    return df_prices,normalized_columns\n",
    "\n",
    "def truncateExtremes(df_prices,training_columns,stats, percent_min, percent_max):\n",
    "    for c in training_columns:\n",
    "        v_min,v_max = [stats.loc[percent_min,c], stats.loc[percent_max,c]]\n",
    "        ix = df_prices[c] < v_min\n",
    "        df_prices.loc[ix,c] = v_min\n",
    "        ix = df_prices[c] > v_max\n",
    "        df_prices.loc[ix,c] = v_max\n",
    "    return df_prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f814f0",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727c4a2",
   "metadata": {},
   "source": [
    "# Statics on the mature stocks\n",
    "min, max, mean, std, percentiles\n",
    "calculate the normalization numbers\n",
    "\n",
    "# Create dataset\n",
    "The dataset must a batch with number of stock = batch_size\n",
    "Each sequence of stockprice (ohlc) return from the dataset must be of the samme sequence_length During the training the network will process the sequence day by day\n",
    "\n",
    "At the beginning of each epoch It must be possible to shuffle the stock It must be possible to shuffle the start date of the stockprices for each stock\n",
    "\n",
    "The dataframe will remain fixed during the training using indirect indexing This will be faster and use less memory for large dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd4195",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_prices = loadShareprices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d5351",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df_prices, stocks, validStocks, inValidStocks = procesSharePrices(df_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistics on the mature stocks\n",
    "stats = df_prices.loc[validStocks,[PREV_CLOSE,OPEN,HIGH,LOW,CLOSE]].describe(percentiles=\\\n",
    "                                                                             [0.0002, 0.01, 0.25, 0.75, 0.99, 0.9998])        \n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e7b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_valid =df_prices.loc[validStocks]\n",
    "print( f\"percentage =>0: {df_valid[CLOSE].ge(0).sum()/len(df_valid)*100}\")\n",
    "print( f\"percentage ==0: {df_valid[CLOSE].eq(0).sum()/len(df_valid)*100}\")\n",
    "print( f\"percentage  <0: {df_valid[CLOSE].lt(0).sum()/len(df_valid)*100}\")\n",
    "\n",
    "print( f\"mean=0 percentage =>0: {(df_valid[CLOSE]-stats.loc['mean',CLOSE]).ge(0).sum()/len(df_valid)*100}\")\n",
    "print( f\"mean=0 percentage ==0: {(df_valid[CLOSE]-stats.loc['mean',CLOSE]).eq(0).sum()/len(df_valid)*100}\")\n",
    "print( f\"mean=0 percentage  <0: {(df_valid[CLOSE]-stats.loc['mean',CLOSE]).lt(0).sum()/len(df_valid)*100}\")\n",
    "del df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcae053",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from lib.learner.learner import*\n",
    "from lib.learner.optimizers import*\n",
    "from lib.model.model import*\n",
    "from lib.model.modelmanager import*\n",
    "import torch.nn as nn\n",
    "\n",
    "training_columns = [CLOSE,OPEN]\n",
    "#mean is so close to zero so we only devide by std\n",
    "stats = df_prices.loc[validStocks].describe(percentiles=[0.0002, 0.01, 0.25, 0.75, 0.99, 0.9998])\n",
    "df_prices, normalized_columns = normalizeData(df_prices,stats)\n",
    "\n",
    "#truncate extrems\n",
    "stats = df_prices.loc[validStocks,normalized_columns].describe(percentiles=[0.0002, 0.01, 0.25, 0.75, 0.99, 0.9998])\n",
    "df_prices = truncateExtremes(df_prices,normalized_columns, stats, \"1%\", \"99%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabc053",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 360\n",
    "training_columns = [CLOSE_NORM,OPEN_NORM]\n",
    "n_predict_columns=1\n",
    "print(f\"seq_length: {seq_length} training columns = {training_columns} n_predict_columns:{n_predict_columns}\")\n",
    "\n",
    "data = Data(df_prices)\n",
    "data.changeTraining(seq_length,training_columns,n_predict_columns=n_predict_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91d361e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ohlc_ds = OHLCDataset(data, stocks=validStocks)\n",
    "train_ds, test_ds = ohlc_ds.split2train_test(0.25)\n",
    "train_ds.changeAugmentation( mixing=0.5 )\n",
    "\n",
    "print(f\"number of stocks, train stocks, test stocks: {len(ohlc_ds.stocks)}, {len(train_ds.stocks)}, {len(test_ds.stocks)}\")\n",
    "\n",
    "databunch = DataBunch(train_ds.dataloader(batch_size=2048, shuffle=True,  drop_last=True), \\\n",
    "                      test_ds.dataloader( batch_size=4096, shuffle=False, drop_last=False), \\\n",
    "                      c_in=len(ohlc_ds.data.columns), c_out=len(ohlc_ds.data.columns))\n",
    "\n",
    "#print(\"the following lengths must be the same\")\n",
    "#%time stock_days = [len(ohlc_ds.stock_grps.get_group(stock)) for stock in ohlc_ds.stocks]\n",
    "#print(len(ohlc_ds), sum(stock_days)-len(stock_days)*(seq_length+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0227757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time l = [len(b[0]) for b in databunch.train_dl]\n",
    "#CPU times: user 1min 52s, sys: 2.09 s, total: 1min 54s\n",
    "#Wall time: 1min 54s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices[[\"idx\",\"ix_predict\"]].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f32b6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted app_image_02_imagenette_optimizers.ipynb.\n",
      "Converted fin_01_candlestick.ipynb.\n",
      "Converted fin_02_simfin_data-Copy1.ipynb.\n",
      "Converted fin_02_simfin_data.ipynb.\n",
      "Converted fin_02_simfin_generated_data.ipynb.\n",
      "Converted fin_02_simfin_training.ipynb.\n",
      "Converted fin_03_graphs.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted parallel.ipynb.\n",
      "Converted parallel_extern.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81599945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
