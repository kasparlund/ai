{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c52aff0a",
   "metadata": {},
   "source": [
    "# TODO fase 1\n",
    " - calculate predictions for a stock fx apple\n",
    " - statistics for the predictions: distribution of errors, percentage prediction with the right direction:  close>open, open<close\n",
    " - visualize ohlc as bands and on stock curve\n",
    " - split in train, test so that test data are taken from time periodens after the training data and possibly from stocks that have not been used for training \n",
    "\n",
    " - tjek quantile objective function\n",
    " - objectiv function that weigh open and closing prices higher\n",
    "\n",
    " - add day of week to training data\n",
    "                                                                                                                 \n",
    "                                                                                                                 \n",
    "# TODO fase 2\n",
    " - language modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5745b4d8",
   "metadata": {},
   "source": [
    "# DONE \n",
    " - speed up training: improve the pandas part by 30 to 60 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c3733f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp finance.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3cd767",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55bdddce",
   "metadata": {},
   "source": [
    "#add the parent directiry so thatwecan access modules the and inits subdirectories\n",
    "import sys, os, inspect\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir  = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec212ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from lib.data.lists import *\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the main functionality from the SimFin Python API.\n",
    "import simfin as sf\n",
    "\n",
    "# Import names used for easy access to SimFin's data-columns.\n",
    "from simfin.names import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78d1707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version of the SimFin Python API.\n",
    "sf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181fd919",
   "metadata": {},
   "source": [
    "# Define location of Simfin data and license"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f825c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def initiateSimFin(key='free'):\n",
    "    # SimFin data-directory.\n",
    "    sf.set_data_dir('~/simfin_data/')\n",
    "    # SimFin load API key or use free data.\n",
    "    sf.load_api_key(path='~/simfin_api_key.txt', default_key=key)\n",
    "    return Path.home()/\"simfin_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba164b",
   "metadata": {},
   "source": [
    "# Prepare data for prediction of next days stock prices the next day\n",
    "Create OHLC price changes as percentages relative to the close of the previous day. Furthermore the ohlc is supplemented by the previous days closing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c9fd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "PREV_CLOSE = \"previous_close\"\n",
    "def alignPrices(df_prices):\n",
    "    # Make a new column to show the closing price from the previous day on the same line as current day. \n",
    "    # This is done using the shift function for the dataserie\n",
    "    # \n",
    "    # Result: All stock and prices are listed in the same tabel. Therefore, the firat priceline of each stock vil now \n",
    "    # contain the close of the previous stock. For the first stock this value vil be \"nan\". \n",
    "    # These incoherent pricelines are removed in the nest step\n",
    "    \n",
    "    #if PREV_CLOSE not in df_prices.columns: \n",
    "    if not PREV_CLOSE in df_prices.columns: \n",
    "        df_prices.insert(df_prices.columns.get_loc(OPEN), PREV_CLOSE, df_prices[CLOSE].shift(), allow_duplicates=False)\n",
    "\n",
    "    #identify the first stock in each stockgroup and the remove it\n",
    "    stock_name    = df_prices.index.get_level_values(0)\n",
    "    new_stock     = np.ones(len(df_prices), dtype=bool)\n",
    "    new_stock[1:] = stock_name[0:len(stock_name)-1] != stock_name[1:len(stock_name)]\n",
    "    df_prices.drop(df_prices.index[new_stock], axis=0, inplace=True)\n",
    "    return df_prices\n",
    "\n",
    "#Convert OHLC to percentages of the previous days closing price\n",
    "def convertToPercentages(df_prices):\n",
    "    df_prices[[OPEN,CLOSE,LOW,HIGH]] = df_prices[[OPEN,CLOSE,LOW,HIGH]].sub(df_prices[PREV_CLOSE],axis=0)\\\n",
    "                                        .div(df_prices[PREV_CLOSE],axis=0)\n",
    "    return df_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c910867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "#   extrem_increase,extrem__decrease = [], 2.5, -.73\n",
    "#   extrem_increase,extrem__decrease = [], 1.5, -.5\n",
    "EXTREM_HL = \"extrem_hl\"\n",
    "def flagstocks_extrem_hl(df_prices, extrem_increase, extrem__decrease):\n",
    "    #remove mature stocks with \"nan\" prices\n",
    "    if EXTREM_HL in df_prices.columns: \n",
    "        df_prices.drop(labels=EXTREM_HL,axis=1,inplace=True)\n",
    "                       \n",
    "    gt_p = df_prices[[HIGH]].gt(extrem_increase)\n",
    "    gt_p.insert(0,\"lt\",df_prices[[LOW]].lt(extrem__decrease))\n",
    "    df_prices.insert(df_prices.columns.get_loc(SIMFIN_ID)+1,EXTREM_HL,gt_p.any(axis=1))    \n",
    "    return df_prices\n",
    "\n",
    "TOO_FEW_DAYS = \"too_few_days\"\n",
    "def flagstocks_too_few_trading_days(df_prices, minimum_tradingdays):\n",
    "    grp_res = df_prices.groupby([\"Ticker\"]).apply(lambda group: len(group) < minimum_tradingdays) \n",
    "    grp_res.name = TOO_FEW_DAYS\n",
    "    merged = df_prices[[SIMFIN_ID]].join(grp_res, on=TICKER)\n",
    "    if TOO_FEW_DAYS in df_prices.columns: \n",
    "        df_prices.drop(labels=TOO_FEW_DAYS,axis=1,inplace=True)\n",
    "    df_prices.insert(df_prices.columns.get_loc(SIMFIN_ID)+1, TOO_FEW_DAYS, merged[TOO_FEW_DAYS].values)\n",
    "    return df_prices\n",
    "\n",
    "HAS_NAN_DAYS = \"has_nan_days\"\n",
    "def flagstocks_with_nan_days(df_prices):\n",
    "    grp_res = df_prices.groupby([\"Ticker\"]).apply(lambda group: group[[OPEN,LOW,HIGH,CLOSE]].isnull().values.any() ) \n",
    "    grp_res.name = HAS_NAN_DAYS\n",
    "    merged = df_prices[[SIMFIN_ID]].join(grp_res, on=TICKER)\n",
    "    if HAS_NAN_DAYS in df_prices.columns: \n",
    "        df_prices.drop(labels=HAS_NAN_DAYS,axis=1,inplace=True)\n",
    "    df_prices.insert(df_prices.columns.get_loc(SIMFIN_ID)+1, HAS_NAN_DAYS, merged[HAS_NAN_DAYS].values)\n",
    "    return df_prices\n",
    "\n",
    "def findInValidStocks(df_prices):\n",
    "    flags  = [HAS_NAN_DAYS,TOO_FEW_DAYS,EXTREM_HL]\n",
    "    groups = df_prices.groupby([\"Ticker\"])\n",
    "    return np.array([name for (name,group) in groups if group[flags].any(axis=1).any()])\n",
    "\n",
    "def findValidStocks(df_prices):\n",
    "    flags  = [HAS_NAN_DAYS,TOO_FEW_DAYS,EXTREM_HL]\n",
    "    groups = df_prices.groupby([\"Ticker\"])\n",
    "    return np.array([name for (name,group) in groups if not group[flags].any(axis=1).any()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303ee3f6",
   "metadata": {},
   "source": [
    "# find valid and invalid stocks\n",
    "#df_prices[df_prices[[HAS_NAN_DAYS,TOO_FEW_DAYS,EXTREM_HL]].any(axis=1)]\n",
    "len(findValidStocks(df_prices)), len(findInValidStocks(df_prices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5451f305",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "The dataset must a batch with number of stock = batch_size  \n",
    "Each sequence of stockprice (ohlc) return from the dataset must be of the samme sequence_length\n",
    "During the training the network will process the sequence day by day\n",
    "\n",
    "At the beginning of each epoch\n",
    "    It must be possible to shuffle the stock \n",
    "    It must be possible to shuffle the start date of the stockprices for each stock \n",
    "\n",
    "The dataframe will remain fixed during the training using indirect indexing \n",
    "This will be faster and use less memory for large dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94937dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 12, array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11.]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test of logic to interleave data from df in np_prices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "open_=np.arange(0,12,4)\n",
    "high =np.arange(1,12,4)\n",
    "low  =np.arange(2,12,4)\n",
    "close=np.arange(3,12,4)\n",
    "\n",
    "df = pd.DataFrame({\"open\":open_,\"high\":high,\"low\":low,\"close\":close})\n",
    "columns=df.columns\n",
    "\n",
    "n_columns=len(columns)\n",
    "len_array=df.shape[0]\n",
    "np_prices = np.empty(len_array*n_columns)\n",
    "\n",
    "for (i,c) in enumerate(columns):\n",
    "    np_prices[i::n_columns] = df[c].values\n",
    "index = 2\n",
    "seq_length = 1\n",
    "index*n_columns, (seq_length+index)*n_columns, np_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from lib.data.lists import *\n",
    "import torch\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "#Create a dataset that uses all the stocks and daily prices \n",
    "#In order to save memory the df_prices is the same for both training and validation.\n",
    "#np_prices contains interleaved prices of the columns used for training/validation\n",
    "#fx if the require columns are \"open\", \"close\". Then np_prices contain row, columns values from \n",
    "#df_price in the following order : open_row1, close_row1, open_row2, close_row2, ...\n",
    "\n",
    "#for now self.np_prices are dublicated. Thus using more memory than needed.\n",
    "#Test and validation contain different stock taken drom the same df_prices\n",
    "\n",
    "#stocks    : are the stock tickers\n",
    "#stock_days: is the number of days pr stock. The number of days is in different for each stock\n",
    "#            The total number of samples is stock_days.sum()\n",
    "#batch_size: is the number of stocks in a batch\n",
    "#seq_length: is the number of stocks days in a batch. \n",
    "#the number of sample in the batch is seq_length*len(column_names)\n",
    "\n",
    "#As for now the dataset progress sequentially through the data during an epoch from a random offset. \n",
    "#Notice that there is no end of sequence-token to reset the model when the sequencen wraps around\n",
    "\n",
    "class OHLCDataset(torch.utils.data.Dataset):\n",
    "    #x, y significes input vs output\n",
    "    def __init__(self, df_prices, stocks, column_names, seq_length): \n",
    "        self.df_prices    = df_prices\n",
    "        self.stock_grps   = self.df_prices.groupby([\"Ticker\"])\n",
    "        self.stocks       = np.array(list(self.stock_grps.groups)) if stocks is None else np.array(stocks)\n",
    "        self.seq_length   = seq_length\n",
    "        self.column_names = column_names\n",
    "        self.ix_columns   = [self.df_prices.columns.get_loc(k) for k in self.column_names]\n",
    "\n",
    "        #Index to all start of sequences in the dataset\n",
    "        #The target is taken from the day after the sequence\n",
    "        self.idx_seq      = None\n",
    "        self.prices       = None\n",
    "\n",
    "    def __len__(self): \n",
    "        if self.idx_seq is None: self.initializeSquenzing()\n",
    "        return len(self.idx_seq)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "        ix = self.idx_seq[index,:]\n",
    "        \n",
    "        #pi = torch.tensor(self.price_values[ix[0]:ix[1]])\n",
    "        #pt = torch.tensor(self.price_values[ix[1]])\n",
    "        pi = torch.tensor( self.np_prices[ix[1]:ix[0]] )\n",
    "        pt = torch.tensor( self.np_prices[ix[0]:ix[0]+len(self.column_names)] )\n",
    "        return pi, pt #, index\n",
    "    \n",
    "    \"\"\"\n",
    "    def __getitem__(self, index):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "        ix = self.idx_seq[index,:]\n",
    "        #pi = torch.tensor(self.price_values[ix[0]:ix[1]])\n",
    "        #pt = torch.tensor(self.price_values[ix[1]])\n",
    "        pi = torch.tensor(self.price_values[ix[1]:ix[0]])\n",
    "        pt = torch.tensor(self.price_values[ix[0]])\n",
    "        return pi, pt #, index\n",
    "    \"\"\"\n",
    "\n",
    "    #stocks: stocks that you want the price_input and target for    \n",
    "    def getBatch(self, stocks):\n",
    "        #pt: price target\n",
    "        #pi: price input\n",
    "        #extract the rows in df_proces containing the stocks in the batch\n",
    "        idx = self.getSequenceIndexing(stocks);\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        df_prices_selected = self.df_prices[ self.df_prices.index.get_level_values(0).isin(stocks) ]\n",
    "        stock_grps         = df_prices_selected.groupby([\"Ticker\"])\n",
    "        idx, seq_len = [], self.seq_length\n",
    "        for name,group in stock_grps:\n",
    "            idx.extend(group[\"idx\"].values[seq_len:len(group)])     \n",
    "        idx = np.array(idx,dtype=int)\n",
    "    \n",
    "        pt = self.df_prices.iloc[idx]\n",
    "        pi = [self.df_prices.iloc[ib:ie, self.ix_columns].values for ib,ie in zip(pt.idx - seq_len, pt.idx)]\n",
    "        return np.array(pi), self.df_prices.iloc[idx].copy()\n",
    "        \"\"\"\n",
    "        \n",
    "    def initializeSquenzing(self):\n",
    "        #Notice that the dataset cannot be used for a RNN where the data should been returned from a sliding window\n",
    "\n",
    "        #Calculate the start and end of all sequences so that the dataloader can load and shuffle efficiently.  \n",
    "        #add an index column to df_prices that we can used to calcualte start and end of sequences\n",
    "        if not \"idx\" in self.df_prices.columns:\n",
    "            self.df_prices.insert(loc=0, column=\"idx\", value=np.arange(len(self.df_prices),dtype=int))\n",
    "    \n",
    "        #create one array \"prioces with interleaved values from training.columns iinterleaved in \n",
    "        #fx if the require columns are \"open\", \"close\". Then prices contain row, columns values from \n",
    "        #df_price in the following order : open_row1, close_row1, open_row2, close_row2, ...\n",
    "        self.np_prices = np.empty(self.df_prices.shape[0]*len(self.column_names))\n",
    "        for (i,c) in enumerate(self.column_names):\n",
    "            self.np_prices[i::len(self.column_names)] = self.df_prices[c].values\n",
    "\n",
    "        self.ix_columns = [self.df_prices.columns.get_loc(k) for k in self.column_names]\n",
    "        self.idx_seq    = self.getSequenceIndexing(self.stocks)\n",
    "        \n",
    "    def getSequenceIndexing(self,stocks):\n",
    "        #get an index to all valid samples in self.prices\n",
    "        idx_begin = [] #list of valid dataset index into the dataframe        \n",
    "        for stock in stocks:\n",
    "            group = self.stock_grps.get_group(stock)\n",
    "            idx_begin.extend( group[\"idx\"].values[self.seq_length:len(group)] )        \n",
    "        idx_begin    = np.array(idx_begin,dtype=int)    \n",
    "        idx_seq      = np.empty( (len(idx_begin),2), dtype=int)  \n",
    "        #idx_seq[:,0] = (idx_begin+self.seq_length)*len(self.column_names)\n",
    "        idx_seq[:,0] = idx_begin*len(self.column_names)\n",
    "        idx_seq[:,1] = idx_seq[:,0] - self.seq_length*len(self.column_names)\n",
    "        return idx_seq\n",
    "        \"\"\"    \n",
    "        #get an index to all valid samples\n",
    "        idx_begin = [] #list of valid dataset index into the dataframe        \n",
    "        for stock in self.stocks:\n",
    "            group = self.stock_grps.get_group(stock)\n",
    "            #idx_begin.extend( group[\"idx\"].values[:len(group)-self.seq_length] )        \n",
    "            idx_begin.extend( group[\"idx\"].values[self.seq_length:len(group)] )        \n",
    "        self.idx_seq = np.empty( (len(idx_begin),2), dtype=int)  \n",
    "        self.idx_seq[:,0] = idx_begin\n",
    "        self.idx_seq[:,1] = self.idx_seq[:,0]-self.seq_length\n",
    "        \"\"\"\n",
    "    def dataloader(self, batch_size:int, shuffle:bool, num_workers:int=0, drop_last=False):\n",
    "        self.initializeSquenzing()\n",
    "        return torch.utils.data.DataLoader(self, batch_size=batch_size, shuffle=shuffle,\n",
    "                                           num_workers=num_workers, drop_last=drop_last)\n",
    "    \n",
    "    def split2train_test(self, test_percentage):\n",
    "        #split the stocks in train and test stocks\n",
    "        #It might be better to make a split in historical vs new prices \n",
    "        ix_all   = np.arange(len(self.stocks),dtype=int)\n",
    "        np.random.shuffle(ix_all)\n",
    "        \n",
    "        nb_test  = int(round(test_percentage*len(self.stocks)))\n",
    "        ix_train = ix_all[nb_test:]\n",
    "        ix_test  = ix_all[:nb_test]\n",
    "        return self.__class__(self.df_prices, self.stocks[ix_train], self.column_names, self.seq_length),\\\n",
    "               self.__class__(self.df_prices, self.stocks[ix_test],  self.column_names, self.seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531ef2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "\n",
    "#extrem_increase,extrem__decrease = 2.5, -.73\n",
    "#extrem_increase,extrem__decrease = 1.5, -.5\n",
    "def prepareSimFinData(minimum_tradingdays=180, extrem_increase = 1.5, extrem__decrease = -0.5):\n",
    "\n",
    "    #Load shareprices for testing and development\n",
    "    dataPath = initiateSimFin(key='free')\n",
    "    print(f\"dataPath:{dataPath} exists:{dataPath.exists()}\")\n",
    "\n",
    "    # Data for USA.\n",
    "    market = 'us'\n",
    "    # Daily Share-Prices.\n",
    "    df_prices = sf.load_shareprices(variant='daily', market=market)\n",
    "    df_prices.head()\n",
    "\n",
    "    # load and identify valid data\n",
    "    alignPrices(df_prices)\n",
    "    convertToPercentages(df_prices)\n",
    "\n",
    "    flagstocks_extrem_hl(df_prices, extrem_increase, extrem__decrease)\n",
    "    flagstocks_too_few_trading_days(df_prices,minimum_tradingdays)\n",
    "    flagstocks_with_nan_days(df_prices)\n",
    "\n",
    "    validStocks, inValidStocks = findValidStocks(df_prices), findInValidStocks(df_prices)\n",
    "\n",
    "    stock_grps = df_prices.groupby([\"Ticker\"])\n",
    "    stocks     = np.array(list(stock_grps.groups))\n",
    "    sizes      = stock_grps.size()\n",
    "    print(f\"number of stocks:         {len(stocks)}\")\n",
    "    print(f\"number of valid stocks:   {len(validStocks)}\")\n",
    "    print(f\"number of invalid stocks: {len(inValidStocks)}\")\n",
    "\n",
    "    print(f\"smallest pricelines pr stock: {sizes.sort_values()[:5]}\")\n",
    "    print(f\"longest pricelines pr stock:  {sizes.sort_values()[-5:]}\")\n",
    "     \n",
    "    return df_prices, stocks, validStocks, inValidStocks\n",
    "\n",
    "predict_prefix=\"predict_\"\n",
    "[PREDICT_OPEN,PREDICT_HIGH,PREDICT_LOW,PREDICT_CLOSE] = [predict_prefix+OPEN,\\\n",
    "                                                         predict_prefix+HIGH,\\\n",
    "                                                         predict_prefix+LOW,\\\n",
    "                                                         predict_prefix+CLOSE]\n",
    "def predict_stocks(dataset, modelmanager, stocks, tfm_input ):\n",
    "    price_sequences, price_targets = dataset.getBatch(stocks=stocks)\n",
    "    predictions    = modelmanager.predict(price_sequences, tfm_input)\n",
    "    \n",
    "    # inser the preduction in price_targets\n",
    "    prediction_columns = [ predict_prefix + name for name in dataset.column_names ]\n",
    "    print(prediction_columns)\n",
    "    for idx,name in enumerate(prediction_columns):\n",
    "        print(idx, name)\n",
    "        if name in price_targets.columns : \n",
    "            price_targets.drop([name], axis='columns', inplace=True)\n",
    "        else:    \n",
    "            price_targets.insert(len(price_targets.columns), name, predictions[:,idx-1].numpy())\n",
    "    return price_targets, prediction_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dbcc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization_faktor = 2.697750e-02\n",
    "#TODO add normalized datacolumn to df_prices indstead of replacing them\n",
    "normalized_prefix=\"normalized_\"\n",
    "[NORM_OPEN,NORM_HIGH,NORM_LOW,NORM_CLOSE] = [normalized_prefix+OPEN,\\\n",
    "                                             normalized_prefix+HIGH,\\\n",
    "                                             normalized_prefix+LOW,\\\n",
    "                                             normalized_prefix+CLOSE]\n",
    "\n",
    "def normalizeData( df_prices, columns, normalization_faktor=2.7e-02, normalized_prefix=normalized_prefix):\n",
    "    #normalize the training data\n",
    "    if not normalization_faktor==1.0 :\n",
    "        normalized_columns = [\"normalized_\"+ name for name in columns]\n",
    "        df_prices[normalized_columns] = df_prices[columns].div(normalization_faktor,axis=\"columns\").values\n",
    "    return df_prices,normalized_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518f76e5",
   "metadata": {},
   "source": [
    "# Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d438cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prices, stocks, validStocks, inValidStocks = prepareSimFinData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3d41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistics on the mature stocks\n",
    "df_prices.loc[validStocks,[PREV_CLOSE,OPEN,HIGH,LOW,CLOSE]].describe(percentiles=[0.0002, 0.25, 0.75, 0.9998])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb1506",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "from lib.learner.learner import*\n",
    "from lib.learner.optimizers import*\n",
    "from lib.model.model import*\n",
    "from lib.model.modelmanager import*\n",
    "import torch.nn as nn\n",
    "\n",
    "seq_length = 32\n",
    "training_column_names = [CLOSE]#[OPEN,HIGH,LOW,CLOSE] #[OPEN,#CLOSE]\n",
    "\n",
    "df_prices, training_column_names = normalizeData(df_prices,training_column_names)\n",
    "\n",
    "#each priceline has 4 datapoint:[OPEN,LOW,HIGH,CLOSE]\n",
    "#batchsize is the number of stocks processed in parallel\n",
    "\n",
    "#ohlc: almost all day to day variation are in the the range -1 to 1.\n",
    "#For now we do not normalize the input. However we will have to do it sooner or later\n",
    "\n",
    "#[PREDICT_OPEN,PREDICT_HIGH,PREDICT_LOW,PREDICT_CLOSE] = [\"predict_\"+OPEN,\"predict_\"+HIGH,\"predict_\"+LOW,\"predict_\"+CLOSE]\n",
    "\n",
    "#self.column_names = [OPEN,HIGH,LOW,CLOSE], [OPEN,CLOSE]\n",
    "ohlc_ds = OHLCDataset(df_prices, stocks=validStocks, column_names=training_column_names, seq_length=seq_length)\n",
    "ohlc_ds.initializeSquenzing()\n",
    "print(f\"training columns = {training_column_names}\")\n",
    "\n",
    "train_ds, test_ds = ohlc_ds.split2train_test(0.25)\n",
    "print(f\"number of stocks, train stocks, test stocks: {len(ohlc_ds.stocks)}, {len(train_ds.stocks)}, {len(test_ds.stocks)}\")\n",
    "\n",
    "databunch = DataBunch(train_ds.dataloader(batch_size=2048, shuffle=True,  drop_last=True), \\\n",
    "                      test_ds.dataloader( batch_size=4096, shuffle=False, drop_last=False), \\\n",
    "                      c_in=len(ohlc_ds.column_names), c_out=len(ohlc_ds.column_names))\n",
    "\n",
    "#batch = next(iter(databunch.train_dl))\n",
    "#batch[0].dtype, batch[0].shape, view(batch[0]).shape\n",
    "\n",
    "%time l = [len(b[0]) for b in databunch.train_dl]\n",
    "\n",
    "#print(\"the following lengths must be the same\")\n",
    "#%time stock_days = [len(ohlc_ds.stock_grps.get_group(stock)) for stock in ohlc_ds.stocks]\n",
    "#print(len(ohlc_ds), sum(stock_days)-len(stock_days)*(seq_length+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9def6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import scipy.special\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure, show\n",
    "from lib.finance.graphs import *\n",
    "\n",
    "data_column = training_column_names[0]\n",
    "stocks      = ohlc_ds.stocks\n",
    "df_select   = df_prices.loc[stocks,data_column]\n",
    "measured    = df_select.values\n",
    "x_min,x_max = -measured.min(), measured.max()\n",
    "x           = np.linspace(x_min,x_max, 100)\n",
    "hist, edges = np.histogram(measured, density=True, bins=100)\n",
    "\n",
    "# Normal Distribution\n",
    "#x = np.linspace(x_min,x_max, 1000)\n",
    "#pdf = 1/(sigma * np.sqrt(2*np.pi)) * np.exp(-(x-mu)**2 / (2*sigma**2))\n",
    "#cdf = (1+scipy.special.erf((x-mu)/np.sqrt(2*sigma**2)))/2\n",
    "\n",
    "p1 = make_plot(\"Histogram for \"+data_column, hist, edges, x)\n",
    "show(gridplot([p1], ncols=1, plot_width=1000, plot_height=400))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c9d1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted app_image_02_imagenette_optimizers.ipynb.\n",
      "Converted fin_01_candlestick.ipynb.\n",
      "Converted fin_02_simfin_data.ipynb.\n",
      "Converted fin_02_simfin_generated_data.ipynb.\n",
      "Converted fin_02_simfin_training.ipynb.\n",
      "Converted fin_03_graphs.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d52580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
