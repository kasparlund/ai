{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp learner.learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from lib.data.lists import *\n",
    "import torch.nn.functional as F\n",
    "import time, math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner\n",
    "\n",
    "> The the messaging structure used to organise and monitor training of an NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class Event():\n",
    "    def __init__(self,learner):\n",
    "        self.learn = learner\n",
    "        \n",
    "\n",
    "from enum import Enum,auto\n",
    "class Stages(Enum):\n",
    "    begin_fit = auto()\n",
    "    begin_epoch = auto()\n",
    "    begin_batch = auto()\n",
    "    begin_preprocessing = auto(),\n",
    "    after_preprocessing = auto(),\n",
    "    begin_prediction = auto()\n",
    "    after_prediction = auto()\n",
    "    begin_loss = auto()\n",
    "    after_loss = auto()\n",
    "    begin_backwards = auto()\n",
    "    after_backwards = auto()\n",
    "    begin_step = auto()\n",
    "    after_step = auto()\n",
    "    after_batch = auto()\n",
    "    after_epoch = auto()\n",
    "    after_fit = auto()\n",
    "    begin_validate = auto()\n",
    "    after_validate = auto()  \n",
    "    begin_train = auto()\n",
    "    after_train = auto()  \n",
    "    \n",
    "train_batch_stages = [Stages.begin_preprocessing, Stages.after_preprocessing, \n",
    "                      Stages.begin_prediction, Stages.after_prediction,\n",
    "                      Stages.begin_loss,       Stages.after_loss,\n",
    "                      Stages.begin_backwards,  Stages.after_backwards,\n",
    "                      Stages.begin_step,       Stages.after_step ]\n",
    "valid_batch_stages = [Stages.begin_preprocessing, Stages.after_preprocessing,\n",
    "                      Stages.begin_prediction, Stages.after_prediction,\n",
    "                      Stages.begin_loss,       Stages.after_loss ]\n",
    "\n",
    "class Messenger():\n",
    "    subscriptions = None    \n",
    "    def __init__(self): self.subscriptions = []\n",
    "        \n",
    "    # cbs is instances of Callback whereas cb_funcs are functions that initializes a Callback \n",
    "    def register( self, cb:Callback ): self.subscriptions.append(cb)\n",
    "        \n",
    "    def register_callback_functions( self, cb_funcs ):\n",
    "        for cbf in cb_funcs: self.register(cbf())\n",
    "        \n",
    "    def notify(self, msg:Stages, event):\n",
    "        for cb in self.subscriptions: \n",
    "            f = getattr(cb, msg.name, None)\n",
    "            if f is not None and not event.learn.stop: \n",
    "                #print(f\"in_train: {event.learn.in_train } callback: {type(cb).__name__}.{msg.name}\")            \n",
    "                f(event)\n",
    "            if event.learn.stop:break\n",
    "                \n",
    "#are we missing a begin_train that match begin_validate??????\n",
    "class Learner():\n",
    "    #public\n",
    "    model    = None\n",
    "    opt      = None\n",
    "    xb       = None\n",
    "    yb       = None\n",
    "    in_train = False\n",
    "    #epoch    = 0\n",
    "    epochs   = 0\n",
    "    loss     = -1\n",
    "    #private\n",
    "    _data    = None\n",
    "    _stop    = None\n",
    "    \n",
    "    def __init__(self, model, data, loss_func):\n",
    "        self.model,self._data,self.loss_func = model,data,loss_func\n",
    "        #for cb in listify(cbs): self.msn.register(cb)\n",
    "        self._stop = False\n",
    "        self.logger = print\n",
    "\n",
    "    def find_subcription_by_cls(self,cls):\n",
    "        for s in self.msn.subscriptions:\n",
    "            if type(s) == cls:return s\n",
    "        \n",
    "    @property\n",
    "    def stop(self): return self._stop\n",
    "    @stop.setter \n",
    "    def stop(self, value): \n",
    "        if not self._stop : self._stop = value \n",
    "\n",
    "    def fit(self, epochs, opt, cb_funcs):\n",
    "        self.epochs, self.loss = epochs, tensor(0.)\n",
    "\n",
    "        self.msn = Messenger()\n",
    "        self.msn.register_callback_functions(cb_funcs)\n",
    "        self.opt = opt\n",
    "\n",
    "        event = Event(self)\n",
    "        try:\n",
    "            self.msn.notify(Stages.begin_fit, event)\n",
    "            for epoch in range(epochs):\n",
    "                if self.stop: break\n",
    "                    \n",
    "                self.epoch = epoch  #due to progressbar\n",
    "                self.msn.notify(Stages.begin_epoch,event)\n",
    "                \n",
    "                self.in_train = True\n",
    "                self.all_batches(self._data.train_dl, train_batch_stages, \n",
    "                                 Stages.begin_train, Stages.after_train)\n",
    "                self.in_train = False\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    self.all_batches(self._data.valid_dl, valid_batch_stages, \n",
    "                                     Stages.begin_validate, Stages.after_validate)\n",
    "                        \n",
    "                self.msn.notify(Stages.after_epoch,event)\n",
    "        except Exception as e: self.exception_handler(e)\n",
    "        finally: self.msn.notify(Stages.after_fit, event)\n",
    "\n",
    "        self.epoch, self.in_train = 0, False\n",
    "                    \n",
    "    def all_batches(self, dl, batch_stages, begin_msg:Stages, after_msg:Stages):\n",
    "        event = Event(self)        \n",
    "        self.dl    = self._data.train_dl #due to progress bar\n",
    "        self.iters = len(dl)\n",
    "\n",
    "        try:\n",
    "            self.msn.notify(begin_msg,event)\n",
    "            #for i,(xb,yb,index) in enumerate(dl): \n",
    "            for i,(xb,yb) in enumerate(dl): \n",
    "                if self.stop: break\n",
    "                self.iter = i\n",
    "                self.one_batch(batch_stages, xb, yb)\n",
    "        except Exception as e: self.exception_handler(e)\n",
    "        finally: self.msn.notify(after_msg,event)\n",
    "        self.dl = None\n",
    "             \n",
    "    def one_batch(self, batch_stages, xb, yb):\n",
    "        event = Event(self)        \n",
    "        self.xb,self.yb = xb,yb\n",
    "        try:\n",
    "            self.msn.notify(Stages.begin_batch,event)\n",
    "            for msg in batch_stages: \n",
    "                self.msn.notify(msg,event)\n",
    "        except Exception as e: self.exception_handler(e)\n",
    "        finally: self.msn.notify(Stages.after_batch,event)\n",
    "        self.xb,self.yb = None,None\n",
    "\n",
    "    def exception_handler(self, e:Exception ):\n",
    "        self.stop = True\n",
    "        import traceback\n",
    "        print(\"exception: {e}\")\n",
    "        tb = traceback.format_exc()            \n",
    "        print(f\"exception received 3\\n:{tb}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#generalize this callback to debug callback\n",
    "class GetOneBatchCallback(Callback):\n",
    "    def after_preprocessing(self, e:Event): \n",
    "        self.xb,self.yb = e.learn.xb,e.learn.yb\n",
    "        e.learn.stop = True\n",
    "\n",
    "class CudaCallback(Callback):\n",
    "    def __init__(self, device): self.device = device\n",
    "    def begin_fit(  self, e:Event): e.learn.model.to(self.device)\n",
    "    def begin_batch(self, e:Event): e.learn.xb, e.learn.yb = e.learn.xb.to(self.device),e.learn.yb.to(self.device)\n",
    "\n",
    "class SimpleCudaCallback(Callback):\n",
    "    def __init__(self, device): super()(device = torch.device('cuda',0))\n",
    "    \n",
    "        \n",
    "class BatchTransformXCallback(Callback):\n",
    "    def __init__(self, tfm): self.tfm = tfm\n",
    "    def begin_batch(self, e:Event): e.learn.xb = self.tfm(e.learn.xb)\n",
    "\n",
    "        \n",
    "#must always be used for training        \n",
    "class TrainableModelCallback(Callback):\n",
    "    def begin_prediction(self,e:Event): e.learn.preds = e.learn.model(e.learn.xb)\n",
    "\n",
    "    def begin_backwards(self,e:Event): \n",
    "        if e.learn.in_train: e.learn.loss.backward()\n",
    "    \n",
    "    def begin_loss(self,e:Event): \n",
    "        e.learn.loss = e.learn.loss_func(e.learn.preds, e.learn.yb)\n",
    "\n",
    "#must always be used for training        \n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_train(self,e:Event): \n",
    "        if e.learn.in_train: e.learn.model.train()\n",
    "\n",
    "    def begin_validate(self, e:Event): e.learn.model.eval()\n",
    "      \n",
    " \n",
    "def accuracy(out, yb): return (torch.argmax(out, dim=1)==yb).float().mean()\n",
    "\n",
    "class AvgStats():\n",
    "    def __init__(self, metrics, in_train): self.metrics,self.in_train = metrics,in_train\n",
    "\n",
    "    def reset(self):\n",
    "        self.tot_loss,self.count = 0.,0\n",
    "        self.tot_mets = [0.] * len(self.metrics)\n",
    "\n",
    "    @property\n",
    "    def all_stats(self): return [self.tot_loss.item()] + self.tot_mets\n",
    "    @property\n",
    "    def avg_stats(self): return [o/self.count for o in self.all_stats]\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self.count: return \"\"\n",
    "        return f\"{'train' if self.in_train else 'valid'}: {self.avg_stats}\"\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        bn = learn.xb.shape[0]\n",
    "        self.tot_loss += learn.loss * bn\n",
    "        self.count += bn\n",
    "        for i,m in enumerate(self.metrics):\n",
    "            self.tot_mets[i] += m(learn.preds, learn.yb) * bn\n",
    "\n",
    "\"\"\"\n",
    "#class DebugCallback(Callback):\n",
    "#    def __init__(self, cb_name, f_cond=None): self.cb_name,self.f_cond = cb_name,f_cond\n",
    "#    def __call__(self, cb_name):\n",
    "#        if cb_name==self.cb_name:\n",
    "#            if self.f_cond: self.f_cond(self.run)\n",
    "#            else:      set_trace()\n",
    "\"\"\"\n",
    "\n",
    "###################################### Hooks ###################################### \n",
    "from functools import partial\n",
    "class Hook():\n",
    "    def __init__(self, layer, func): \n",
    "        self.layer, self.hook = layer, layer.register_forward_hook(partial(func, self))\n",
    "    def remove(self): self.hook.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "\n",
    "class Hooks(ListContainer):\n",
    "    def __init__(self, modules, f): \n",
    "        super().__init__([Hook(m, f) for m in modules])\n",
    "            \n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__ (self, *args): self.remove()\n",
    "    def __del__(self): self.remove()\n",
    "\n",
    "    def __delitem__(self, i):\n",
    "        self[i].remove()\n",
    "        super().__delitem__(i)\n",
    "        \n",
    "    def remove(self):\n",
    "        for h in self: h.remove()\n",
    "\n",
    "class HookCallback(Callback):   \n",
    "    def __init__(self, hookProcessor): self.hookProcessor = hookProcessor\n",
    "\n",
    "    def begin_fit(self, e:Event):\n",
    "        self.lrs = [[] for _ in e.learn.opt.param_groups]\n",
    "        self.losses = []\n",
    "\n",
    "    def after_batch(self, e:Event):\n",
    "        if e.learn.in_train:         \n",
    "            for pg,lr in zip(e.learn.opt.param_groups, self.lrs): lr.append(pg['lr'])\n",
    "            self.losses.append(e.learn.loss.detach().cpu())\n",
    "\n",
    "            \n",
    "def plot_layer_stats( hooks:Hooks, pct_lower_bins = 2 ):\n",
    "    def get_hist(h): return torch.stack(h.stats[2]).t().float().log1p()\n",
    "\n",
    "    rows = int( len(hooks)/2 + 0.5)\n",
    "    fig,axes = plt.subplots(rows,2, figsize=(15,3*rows))\n",
    "    for i,ax,h in zip(range(len(hooks)),axes.flatten(), hooks):\n",
    "        ax.imshow(get_hist(h), origin='lower', aspect=\"auto\", interpolation=\"bicubic\")\n",
    "        ax.set(xlabel='iterations', ylabel=\"histogram\", title=f\"l:{i}, {type(h.layer)}: ln(output + 1)\")  \n",
    "        #ax.set_axis_off()\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    fig,axes = plt.subplots(rows,2, figsize=(15,3*rows))\n",
    "    sds = np.fromiter( h.stats[2] for h in hooks, dtype=np.float32, count=len(hooks))\n",
    "    sds_mean = np.mean(sds)\n",
    "\n",
    "    for i,ax,h in zip(range(len(hooks)),axes.flatten(), hooks):\n",
    "        ax.plot( get_min2(h, 0.1 * sds_mean) )\n",
    "        #ax.plot( get_min(h,pct_lower_bins) )\n",
    "        ax.set_ylim(0,100)\n",
    "        ax.set(xlabel='iterations', ylabel=\"% near zero\",  title=f\"layer {i}, {type(h.layer)}: output near zero\")  \n",
    "    plt.tight_layout()    \n",
    "    \"\"\"\n",
    "\n",
    "    fig,(ax0,ax1) = plt.subplots(1,2, figsize=(15,6))\n",
    "    for h in hooks:\n",
    "        ms,ss = h.stats[:2]\n",
    "        ax0.plot(ms)\n",
    "        ax0.set(xlabel='iterations', ylabel=\"mean activation\",  title=f\"mean of activations pr layers\")  \n",
    "        ax0.legend(range(len(hooks)));\n",
    "        ax1.plot(ss)\n",
    "        ax1.set(xlabel='iterations', ylabel=\"std activation\",  title=f\"std of activations pr layers\")  \n",
    "        ax1.legend(range(len(hooks)));\n",
    "    plt.tight_layout()     \n",
    "\n",
    "\n",
    "def append_stats(hook, module, inp, outp, max_activation=5):\n",
    "    if module.training:\n",
    "        if not hasattr(hook,'stats'): hook.stats = ([],[],[])\n",
    "        means,stds,hists = hook.stats\n",
    "        means.append(outp.data.mean().cpu())\n",
    "        stds .append(outp.data.std().cpu())\n",
    "        hists.append(outp.data.cpu().histc(100,-max_activation,max_activation)) #histc isn't implemented on the GPU\n",
    "\n",
    "\n",
    "class AvgStatsCallback(Callback):\n",
    "    def __init__(self, metrics):\n",
    "        self.train_stats, self.valid_stats = AvgStats(metrics,True), AvgStats(metrics,False)\n",
    "        self.first=True\n",
    "\n",
    "    def begin_epoch(self, e:Event):        \n",
    "        self.train_stats.reset()\n",
    "        self.valid_stats.reset()\n",
    "        self.start_time = time.time()\n",
    "        if self.first:\n",
    "            met_names = ['loss'] + [m.__name__ for m in self.train_stats.metrics]\n",
    "            names = ['epoch'] + [f'train_{n}' for n in met_names] + [\n",
    "                    f'valid_{n}' for n in met_names] + ['time']\n",
    "            e.learn.logger(names)\n",
    "            self.first = False\n",
    "\n",
    "    def after_loss(self, e:Event):\n",
    "        stats = self.train_stats if e.learn.in_train else self.valid_stats\n",
    "        with torch.no_grad(): stats.accumulate(e.learn)\n",
    "\n",
    "    def after_epoch(self, e:Event):\n",
    "        #print(self.train_stats)\n",
    "        #print(self.valid_stats)\n",
    "        stats = [str(e.learn.epoch)] \n",
    "        for o in [self.train_stats, self.valid_stats]:\n",
    "            stats += [f'{v:.6f}' for v in o.avg_stats] \n",
    "        stats += [format_time(time.time() - self.start_time)]\n",
    "        e.learn.logger(stats)\n",
    "        \n",
    "from IPython.display import display, Javascript\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from fastprogress.fastprogress import format_time\n",
    "import matplotlib.pyplot as plt\n",
    "class ProgressCallback(Callback):\n",
    "    def begin_fit(self,e:Event):\n",
    "        self.mbar = master_bar(range(e.learn.epochs))\n",
    "        #self.mbar.on_iter_begin()\n",
    "        e.learn.logger = partial(self.mbar.write, table=True)\n",
    "        \n",
    "    def after_fit(self,e:Event): self.mbar.on_iter_end()\n",
    "    def after_batch(self,e:Event): self.pb.update(e.learn.iter)\n",
    "    def begin_train   (self,e:Event): self.set_pb(e.learn)\n",
    "    def begin_validate(self,e:Event): self.set_pb(e.learn)\n",
    "    def set_pb(self,learn:Learner):\n",
    "        self.pb = progress_bar(learn.dl, parent=self.mbar)\n",
    "        self.mbar.update(learn.epoch)   \n",
    "        #plot_loss_update(learn.epoch, e.learn.epochs,   )\n",
    "     \n",
    "    \"\"\"\n",
    "    def plot_loss_update(epoch, epochs, mb, train_loss, valid_loss):\n",
    "        # dynamically print the loss plot during the training/validation loop.\n",
    "        # expects epoch to start from 1.\n",
    "        x = range(1, epoch+1)\n",
    "        y = np.concatenate((train_loss, valid_loss))\n",
    "        graphs = [[x,train_loss], [x,valid_loss]]\n",
    "        x_margin = 0.2\n",
    "        y_margin = 0.05\n",
    "        x_bounds = [1-x_margin, epochs+x_margin]\n",
    "        y_bounds = [np.min(y)-y_margin, np.max(y)+y_margin]\n",
    "\n",
    "        mb.update_graph(graphs, x_bounds, y_bounds)\n",
    "    \"\"\"\n",
    "        \n",
    "class Recorder(Callback):\n",
    "    def __init__(self):\n",
    "        self.lrs, self.train_losses, self.valid_losses = None, None, None\n",
    "        self.optimizers = None\n",
    "        self.epochs = 0\n",
    "        \n",
    "    def begin_fit(self, e:Event):\n",
    "        self.lrs,self.train_losses, self.valid_losses = [],[],[]\n",
    "        self.optimizers = None\n",
    "        self.epochs = e.learn.epochs        \n",
    "        \n",
    "    def begin_epoch(self, e:Event):        \n",
    "        self.valid_losses.append(0)\n",
    "        self.valid_iterations = 0\n",
    "            \n",
    "    def after_batch(self, e:Event):\n",
    "        if e.learn.in_train:         \n",
    "            if self.optimizers is None : \n",
    "                self.optimizers = { k:[v] for k,v in e.learn.opt.getOptimizers().items() }\n",
    "            else:                        \n",
    "                for k,v in e.learn.opt.getOptimizers().items(): self.optimizers[k].append(v)            \n",
    "            self.train_losses.append(e.learn.loss.detach().cpu())\n",
    "        else:\n",
    "            self.valid_losses[e.learn.epoch] += e.learn.loss.detach().cpu() * e.learn.xb.shape[0]\n",
    "            self.valid_iterations            += e.learn.xb.shape[0]\n",
    "            \n",
    "    def after_epoch(self, e:Event):  \n",
    "        if self.valid_iterations > 0:\n",
    "            self.valid_losses[e.learn.epoch] /= self.valid_iterations\n",
    "            \n",
    "    def plot_lr(self):\n",
    "        fig, ax = plt.subplots()\n",
    "        for k,v in self.optimizers.items(): ax.plot(v,label=k)\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set(xlabel='iteration', ylabel='optimizer', title='optimizers')  \n",
    "            \n",
    "    def plot_loss(self, skip_start=0, skip_end=0 ): \n",
    "        #resample validation losses so the slicin works\n",
    "        if self.train_losses is None or len(self.train_losses)==0:\n",
    "            print(\"no losses to plot\")\n",
    "            return\n",
    "        else:\n",
    "            #print(f\"self.train_losses:{self.train_losses}\")\n",
    "            fig, ax = plt.subplots()\n",
    "            s           = slice(skip_start,-skip_end) if skip_end>0 else slice(skip_start, None)\n",
    "            ticksize    = int(len(self.train_losses)/self.epochs)\n",
    "            tick_labels = [i for i in range(1,self.epochs+1)]\n",
    "            tick_pos    = [i*ticksize for i in tick_labels]\n",
    "            l1 = ax.plot(list(range(len(self.train_losses)))[s],self.train_losses[s],label=\"training\")\n",
    "            l2 = ax.plot(tick_pos[s],self.valid_losses[s],label=\"validation\")\n",
    "            plt.xticks(tick_pos[s],tick_labels[s])    \n",
    "            ax.set(xlabel='epochs', ylabel=\"losses\")  \n",
    "            ax.legend()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "len(ds_train):60000, len(ds_test):10000 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.image.image import *\n",
    "from lib.data.external import *\n",
    "\n",
    "#config  = IMAGENETTE_160_Configuration()        \n",
    "config   = MNIST_Configuration()\n",
    "files    = ImageList.from_files( untar_data(config.url) )\n",
    "inputTfm = Image2TensorImage()\n",
    "ds       = ImageDataset(config, files, inputTfm )\n",
    "ds_train, ds_test = ds.split2train_test()\n",
    "print(len(files)), print(f\"len(ds_train):{len(ds_train)}, len(ds_test):{len(ds_test)} \")\n",
    "dl_train, dl_test = ds_train.dataloader(128,True), ds_test.dataloader(512, False)\n",
    "databunch = DataBunch(dl_train, dl_test, config.channels_in, config.channels_out)\n",
    "\n",
    "len(ds), len(ds_train), len(ds_test),len(dl_train), len(dl_test), \n",
    "#ds_train[10], \n",
    "len(next(iter(ds_train)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# load data from mnist or imagenette files\n",
    "from lib.data.external import *\n",
    "from lib.data.images import *\n",
    "import numpy as np\n",
    "\n",
    "path, bs_train, bs_test   = untar_data(URLs.MNIST), 128, 512\n",
    "#path  = untar_data(URLs.IMAGENETTE_160)\n",
    "path, scale, mean, std, bs_train, bs_test  = untar_data(URLs.IMAGENETTE_160), \\\n",
    "                            1./255., [0.485, 0.456, 0.406], [0.229, 0.224, 0.225], \\\n",
    "                            128, 512\n",
    "files  = ImageList.from_files( path )\n",
    "print( f\"path:{path}\\nnb-files: {len(files)} Image size: {files[0].image.size}\" )"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ibx_train      = files.label_by_func( lambda path: path.parent.parent.name==\"training\" )\n",
    "labels         = files.label_by_func( lambda path: int(path.parent.name) )\n",
    "uniques_labels = labels.unique()\n",
    "train,test     = files.split2ways(ibx_train)\n",
    "train_labels, test_labels = labels.split2ways(ibx_train)\n",
    "ds_train       = Dataset(train, train_labels )\n",
    "ds_test        = Dataset(test,  test_labels  )\n",
    "databunch      = DataBunch(ds_train, ds_test, bs_train, bs_test, c_in=1, c_out=len(uniques_labels))\n",
    "dl_train, dl_test = databunch.train_dl(), databunch.valid_dl() \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(uniques_labels)\n",
    "type(files), type(train), type(test), type(train_labels), type(test_labels), len(files), len(train), len(test), len(train_labels), len(test_labels), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.tensor as tensor\n",
    "from functools import *\n",
    "\n",
    "def view_tfm(*size):\n",
    "    def _inner(x): return x.view(*((-1,)+size))\n",
    "    return _inner\n",
    "    \n",
    "class OptimizerCallback(Callback):\n",
    "    def begin_fit(self,e:Event):\n",
    "        #count iteration to adjust the training params to the progress in the training cycle\n",
    "        self.n_iter  = 0      \n",
    "        self.params  = [ p for p in e.learn.model.parameters() if p.requires_grad ]\n",
    "        #self.mov_avg = [ p*0 for p in self.params ]\n",
    "\n",
    "    def begin_batch(self,e:Event): \n",
    "        if e.learn.in_train:\n",
    "            self.fractional_cycle = min(1.,self.n_iter /(e.learn.iters * e.learn.epochs))\n",
    "            e.learn.opt.update(self.fractional_cycle)\n",
    "        \n",
    "    def begin_step(self, e:Event):\n",
    "        if e.learn.in_train:\n",
    "            #for p in self.params: e.learn.opt.optimize(p)\n",
    "            e.learn.opt.optimize(self.params)\n",
    "            #self.mom = 0.9    \n",
    "            #self.mov_avg = self.mov_avg*self.mom + (1-self.mom) *p.grad.data\n",
    "            #state['grad_avg'].mul_(mom).add_(state['mom_damp'], p.grad.data)\n",
    "           \n",
    "    def after_step(self, e:Event):            \n",
    "        if e.learn.in_train:\n",
    "            for p in self.params:\n",
    "                p.grad.detach_()\n",
    "                p.grad.zero_()\n",
    "                \n",
    "    def after_batch(self,e:Event): \n",
    "        if e.learn.in_train: self.n_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_view   = view_tfm(1,28,28)\n",
    "layers_sizes = [8,16,32,32]\n",
    "loss_func    = F.cross_entropy\n",
    "#sched        = combine_scheds([0.3, 0.7], [sched_cos(0.3, 0.6), sched_cos(0.6, 0.2)]) \n",
    "\n",
    "cbfs         = [TrainableModelCallback, TrainEvalCallback, OptimizerCallback, \n",
    "#                partial(CudaCallback, device= torch.device('cuda',0)),\n",
    "#                partial(ParamScheduler, 'lr', sched),\n",
    "                partial(BatchTransformXCallback, tfm = mnist_view), \n",
    "#                partial(MixUp,Î±=0.4),\n",
    "#                LR_Finder,\n",
    "                Recorder, \n",
    "                partial(AvgStatsCallback,[accuracy]),\n",
    "                ProgressCallback\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test that the tranformations works\n",
    "from lib.image.image import *\n",
    "image     = Image.as_tensorimage( files[1] )\n",
    "affines   = AffineTransforms([Rotation(.3,30), ShiftScale(.3,shift=0.5,scale=0.25)])\n",
    "image_tfm = affines(image)\n",
    "\n",
    "gph = Graphics()\n",
    "gph.show_image(files[1] )\n",
    "gph.show()\n",
    "gph.show_image(Image.from_tensorimage(image_tfm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 00_test.ipynb.\n",
      "Converted 01_data.external.ipynb.\n",
      "Converted 02_lists.ipynb.\n",
      "Converted 03_images.ipynb.\n",
      "Converted 04_databunchs_undone.ipynb.\n",
      "Converted 05_Learner.ipynb.\n",
      "Converted 05_model.ipynb.\n",
      "Converted 06_modelmanger.ipynb.\n",
      "Converted 07_optimizers.ipynb.\n",
      "Converted app_image_01_mnist_optimizers.ipynb.\n",
      "Converted augmentation_cpu.ipynb.\n",
      "Converted data_block.ipynb.\n",
      "Converted imagenette_optimizers.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted mnist_experiments.ipynb.\n",
      "Converted mnist_initi_batchnorm.ipynb.\n",
      "Converted transfer_learning.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
